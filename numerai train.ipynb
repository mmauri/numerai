{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "from lightgbm import LGBMRegressor\r\n",
    "import gc\r\n",
    "from numerapi import NumerAPI\r\n",
    "from halo import Halo\r\n",
    "from utils import save_model, load_model, neutralize, get_biggest_change_features, validation_metrics, download_data\r\n",
    "\r\n",
    "\r\n",
    "napi = NumerAPI()\r\n",
    "spinner = Halo(text='', spinner='dots')\r\n",
    "\r\n",
    "current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament\r\n",
    "print(current_round)\r\n",
    "\r\n",
    "# read in all of the new datas\r\n",
    "# tournament data and example predictions change every week so we specify the round in their names\r\n",
    "# training and validation data only change periodically, so no need to download them over again every single week\r\n",
    "napi.download_dataset(\"numerai_training_data_int8.parquet\", \"numerai_training_data_int8.parquet\")\r\n",
    "napi.download_dataset(\"numerai_tournament_data_int8.parquet\", f\"numerai_tournament_data_{current_round}_int8.parquet\")\r\n",
    "napi.download_dataset(\"numerai_validation_data_int8.parquet\", f\"numerai_validation_data_int8.parquet\")\r\n",
    "napi.download_dataset(\"example_predictions.parquet\", f\"example_predictions_{current_round}.parquet\")\r\n",
    "napi.download_dataset(\"example_validation_predictions.parquet\", \"example_validation_predictions.parquet\")\r\n",
    "\r\n",
    "spinner.start('Reading parquet data')\r\n",
    "training_data = pd.read_parquet('numerai_training_data_int8.parquet')\r\n",
    "tournament_data = pd.read_parquet(f'numerai_tournament_data_{current_round}_int8.parquet')\r\n",
    "validation_data = pd.read_parquet('numerai_validation_data_int8.parquet')\r\n",
    "example_preds = pd.read_parquet(f'example_predictions_{current_round}.parquet')\r\n",
    "validation_preds = pd.read_parquet('example_validation_predictions.parquet')\r\n",
    "spinner.succeed()\r\n",
    "\r\n",
    "EXAMPLE_PREDS_COL = \"example_preds\"\r\n",
    "validation_data[EXAMPLE_PREDS_COL] = validation_preds[\"prediction\"]\r\n",
    "\r\n",
    "TARGET_COL = \"target\"\r\n",
    "ERA_COL = \"era\"\r\n",
    "\r\n",
    "# all feature columns start with the prefix \"feature_\"\r\n",
    "feature_cols = [c for c in training_data if c.startswith(\"feature_\")]\r\n",
    "\r\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "283\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-25 21:58:14,763 INFO numerapi.utils: target file already exists\n",
      "2021-09-25 21:58:14,764 INFO numerapi.utils: download complete\n",
      "2021-09-25 21:58:15,684 INFO numerapi.utils: starting download\n",
      "numerai_tournament_data_283_int8.parquet: 582MB [00:14, 40.3MB/s]                           \n",
      "2021-09-25 21:58:31,107 INFO numerapi.utils: target file already exists\n",
      "2021-09-25 21:58:31,108 INFO numerapi.utils: download complete\n",
      "2021-09-25 21:58:32,009 INFO numerapi.utils: starting download\n",
      "example_predictions_283.parquet: 33.5MB [00:01, 17.8MB/s]                            \n",
      "2021-09-25 21:58:34,792 INFO numerapi.utils: target file already exists\n",
      "2021-09-25 21:58:34,794 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "v Reading parquet data\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 1
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "f=['feature_haziest_lifelike_horseback', 'feature_glare_factional_assessment', 'feature_exorbitant_myeloid_crinkle', 'feature_travelled_semipermeable_perruquier', 'feature_branched_dilatory_sunbelt', 'feature_moralistic_heartier_typhoid', 'feature_introvert_symphysial_assegai', 'feature_gullable_sanguine_incongruity', 'feature_agile_unrespited_gaucho', 'feature_canalicular_peeling_lilienthal', 'feature_unvaried_social_bangkok', 'feature_lofty_acceptable_challenge', 'feature_grandmotherly_circumnavigable_homonymity', 'feature_undivorced_unsatisfying_praetorium', 'feature_unaired_operose_lactoprotein']\r\n",
    "f+=['feature_travelled_semipermeable_perruquier', 'feature_planned_superimposed_bend', 'feature_moralistic_heartier_typhoid', 'feature_crowning_frustrate_kampala', 'feature_unaired_operose_lactoprotein', 'feature_flintier_enslaved_borsch', 'feature_cambial_bigoted_bacterioid', 'feature_jerkwater_eustatic_electrocardiograph', 'feature_unvaried_social_bangkok', 'feature_communicatory_unrecommended_velure', 'feature_lofty_acceptable_challenge', 'feature_grandmotherly_circumnavigable_homonymity', 'feature_antichristian_slangiest_idyllist', 'feature_assenting_darn_arthropod', 'feature_haziest_lifelike_horseback', 'feature_exorbitant_myeloid_crinkle', 'feature_beery_somatologic_elimination', 'feature_silver_handworked_scauper', 'feature_canalicular_peeling_lilienthal', 'feature_undivorced_unsatisfying_praetorium']\r\n",
    "f+=['feature_glare_factional_assessment', 'feature_travelled_semipermeable_perruquier', 'feature_moralistic_heartier_typhoid', 'feature_stylistic_honduran_comprador', 'feature_crowning_frustrate_kampala', 'feature_unaired_operose_lactoprotein', 'feature_flintier_enslaved_borsch', 'feature_unvaried_social_bangkok', 'feature_apomictical_motorized_vaporisation', 'feature_lofty_acceptable_challenge', 'feature_antichristian_slangiest_idyllist', 'feature_store_apteral_isocheim', 'feature_unforbidden_highbrow_kafir', 'feature_buxom_curtained_sienna', 'feature_haziest_lifelike_horseback', 'feature_exorbitant_myeloid_crinkle', 'feature_silver_handworked_scauper', 'feature_canalicular_peeling_lilienthal', 'feature_introvert_symphysial_assegai', 'feature_univalve_abdicant_distrail', 'feature_undivorced_unsatisfying_praetorium']\r\n",
    "f+=['feature_glare_factional_assessment', 'feature_unsealed_suffixal_babar', 'feature_travelled_semipermeable_perruquier', 'feature_moralistic_heartier_typhoid', 'feature_twisty_adequate_minutia', 'feature_flintier_enslaved_borsch', 'feature_slack_calefacient_tableau', 'feature_bhutan_imagism_dolerite', 'feature_unvaried_social_bangkok', 'feature_communicatory_unrecommended_velure', 'feature_lofty_acceptable_challenge', 'feature_grandmotherly_circumnavigable_homonymity', 'feature_chuffier_analectic_conchiolin', 'feature_antichristian_slangiest_idyllist', 'feature_unwonted_trusted_fixative', 'feature_haziest_lifelike_horseback', 'feature_exorbitant_myeloid_crinkle', 'feature_beery_somatologic_elimination', 'feature_winsome_irreproachable_milkfish', 'feature_gullable_sanguine_incongruity', 'feature_silver_handworked_scauper', 'feature_canalicular_peeling_lilienthal', 'feature_introvert_symphysial_assegai', 'feature_undivorced_unsatisfying_praetorium']\r\n",
    "\r\n",
    "feature_cols = list(set(f))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "model_name = f\"model_target\"\r\n",
    "print(f\"predicting {model_name}\")\r\n",
    "model = load_model(model_name)\r\n",
    "if not model:\r\n",
    "    print(f\"model not found, training new one\")\r\n",
    "    params = {\"n_estimators\": 2000,\r\n",
    "              \"learning_rate\": 0.01,\r\n",
    "              \"max_depth\": 5,\r\n",
    "              \"num_leaves\": 2 ** 5,\r\n",
    "              \"colsample_bytree\": 0.1}\r\n",
    "\r\n",
    "    model = LGBMRegressor(**params)\r\n",
    "\r\n",
    "    # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\r\n",
    "    spinner.start('Training model')\r\n",
    "    model.fit(training_data.loc[:, feature_cols], training_data[TARGET_COL])\r\n",
    "    print(f\"saving new model: {model_name}\")\r\n",
    "    save_model(model, model_name)\r\n",
    "    spinner.succeed()\r\n",
    "\r\n",
    "# check for nans and fill nans\r\n",
    "if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\r\n",
    "    cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\r\n",
    "    total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\r\n",
    "    print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\r\n",
    "    print(f\"out of {total_rows} total rows\")\r\n",
    "    print(f\"filling nans with 0.5\")\r\n",
    "    tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\r\n",
    "else:\r\n",
    "    print(\"No nans in the features this week!\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicting model_target\n",
      "No nans in the features this week!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# predict on the latest data!\r\n",
    "spinner.start('Predicting on latest data')\r\n",
    "# double check the feature that the model expects vs what is available\r\n",
    "# this prevents our pipeline from failing if Numerai adds more data and we don't have time to retrain!\r\n",
    "model_expected_features = model.booster_.feature_name()\r\n",
    "if set(model_expected_features) != set(feature_cols):\r\n",
    "    print(f\"New features are available! Might want to retrain model {model_name}.\")\r\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(validation_data.loc[:, model_expected_features])\r\n",
    "tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(tournament_data.loc[:, model_expected_features])\r\n",
    "spinner.succeed()\r\n",
    "\r\n",
    "spinner.start('Neutralizing to risky features')\r\n",
    "# getting the per era correlation of each feature vs the target\r\n",
    "all_feature_corrs = training_data.groupby(ERA_COL).apply(lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\r\n",
    "\r\n",
    "# find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\r\n",
    "riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\r\n",
    "\r\n",
    "# neutralize our predictions to the riskiest features\r\n",
    "validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=validation_data,\r\n",
    "                                                                        columns=[f\"preds_{model_name}\"],\r\n",
    "                                                                        neutralizers=riskiest_features,\r\n",
    "                                                                        proportion=0.8,\r\n",
    "                                                                        normalize=True,\r\n",
    "                                                                        era_col=ERA_COL)\r\n",
    "\r\n",
    "tournament_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=tournament_data,\r\n",
    "                                                                        columns=[f\"preds_{model_name}\"],\r\n",
    "                                                                        neutralizers=riskiest_features,\r\n",
    "                                                                        proportion=0.8,\r\n",
    "                                                                        normalize=True,\r\n",
    "                                                                        era_col=ERA_COL)\r\n",
    "spinner.succeed()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<halo.halo.Halo at 0x2404faad880>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "validation_stats"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sharpe</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>apy</th>\n",
       "      <th>max_feature_exposure</th>\n",
       "      <th>feature_neutral_mean</th>\n",
       "      <th>tb200_mean</th>\n",
       "      <th>tb200_std</th>\n",
       "      <th>tb200_sharpe</th>\n",
       "      <th>mmc_mean</th>\n",
       "      <th>corr_plus_mmc_sharpe</th>\n",
       "      <th>corr_with_example_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>preds_model_target_neutral_riskiest_50</th>\n",
       "      <td>0.022826</td>\n",
       "      <td>0.02164</td>\n",
       "      <td>1.054795</td>\n",
       "      <td>-0.085424</td>\n",
       "      <td>198.898264</td>\n",
       "      <td>0.20385</td>\n",
       "      <td>0.013997</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.062751</td>\n",
       "      <td>1.054795</td>\n",
       "      <td>0.008215</td>\n",
       "      <td>0.946037</td>\n",
       "      <td>0.481133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            mean      std    sharpe  \\\n",
       "preds_model_target_neutral_riskiest_50  0.022826  0.02164  1.054795   \n",
       "\n",
       "                                        max_drawdown         apy  \\\n",
       "preds_model_target_neutral_riskiest_50     -0.085424  198.898264   \n",
       "\n",
       "                                        max_feature_exposure  \\\n",
       "preds_model_target_neutral_riskiest_50               0.20385   \n",
       "\n",
       "                                        feature_neutral_mean  tb200_mean  \\\n",
       "preds_model_target_neutral_riskiest_50              0.013997      0.0412   \n",
       "\n",
       "                                        tb200_std  tb200_sharpe  mmc_mean  \\\n",
       "preds_model_target_neutral_riskiest_50   0.062751      1.054795  0.008215   \n",
       "\n",
       "                                        corr_plus_mmc_sharpe  \\\n",
       "preds_model_target_neutral_riskiest_50              0.946037   \n",
       "\n",
       "                                        corr_with_example_preds  \n",
       "preds_model_target_neutral_riskiest_50                 0.481133  "
      ]
     },
     "metadata": {},
     "execution_count": 8
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\r\n",
    "# rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\r\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\r\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\r\n",
    "validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}.csv\")\r\n",
    "tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}.csv\")\r\n",
    "\r\n",
    "# get some stats about each of our models to compare...\r\n",
    "# fast_mode=True so that we skip some of the stats that are slower to calculate\r\n",
    "validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\r\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|                                        |      mean |   sharpe |\n",
      "|:---------------------------------------|----------:|---------:|\n",
      "| preds_model_target_neutral_riskiest_50 | 0.0228259 |  1.05479 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##XGBoost"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from xgboost import XGBRegressor\r\n",
    "\r\n",
    "model_name = f\"xgboost_bare\"\r\n",
    "print(f\"predicting {model_name}\")\r\n",
    "model = load_model(model_name)\r\n",
    "if not model:\r\n",
    "    print(f\"model not found, training new one\")\r\n",
    "    params = {\"n_estimators\": 2000,\r\n",
    "              \"learning_rate\": 0.01,\r\n",
    "              \"max_depth\": 5,\r\n",
    "              \"num_leaves\": 2 ** 5,\r\n",
    "              \"colsample_bytree\": 0.1}\r\n",
    "\r\n",
    "    model = XGBRegressor(max_depth=5, learning_rate=0.01, \\\r\n",
    "                     n_estimators=2000, colsample_bytree=0.1) #provar regularitzacions\r\n",
    "\r\n",
    "    # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\r\n",
    "    spinner.start('Training model')\r\n",
    "    model.fit(training_data.loc[:, feature_cols], training_data[TARGET_COL])\r\n",
    "    print(f\"saving new model: {model_name}\")\r\n",
    "    save_model(model, model_name)\r\n",
    "    spinner.succeed()\r\n",
    "\r\n",
    "# check for nans and fill nans\r\n",
    "if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\r\n",
    "    cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\r\n",
    "    total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\r\n",
    "    print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\r\n",
    "    print(f\"out of {total_rows} total rows\")\r\n",
    "    print(f\"filling nans with 0.5\")\r\n",
    "    tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\r\n",
    "else:\r\n",
    "    print(\"No nans in the features this week!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicting xgboost_bare\n",
      "model not found, training new one\n",
      "\\ Training modelsaving new model: xgboost_bare\n",
      "v Training model\n",
      "No nans in the features this week!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# predict on the latest data!\r\n",
    "spinner.start('Predicting on latest data')\r\n",
    "# double check the feature that the model expects vs what is available\r\n",
    "# this prevents our pipeline from failing if Numerai adds more data and we don't have time to retrain!\r\n",
    "model_expected_features = model.get_booster().feature_names\r\n",
    "if set(model_expected_features) != set(feature_cols):\r\n",
    "    print(f\"New features are available! Might want to retrain model {model_name}.\")\r\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(validation_data.loc[:, model_expected_features])\r\n",
    "tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(tournament_data.loc[:, model_expected_features])\r\n",
    "spinner.succeed()\r\n",
    "\r\n",
    "spinner.start('Neutralizing to risky features')\r\n",
    "# getting the per era correlation of each feature vs the target\r\n",
    "all_feature_corrs = training_data.groupby(ERA_COL).apply(lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\r\n",
    "\r\n",
    "# find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\r\n",
    "riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\r\n",
    "\r\n",
    "# neutralize our predictions to the riskiest features\r\n",
    "validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=validation_data,\r\n",
    "                                                                        columns=[f\"preds_{model_name}\"],\r\n",
    "                                                                        neutralizers=riskiest_features,\r\n",
    "                                                                        proportion=0.8,\r\n",
    "                                                                        normalize=True,\r\n",
    "                                                                        era_col=ERA_COL)\r\n",
    "\r\n",
    "tournament_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=tournament_data,\r\n",
    "                                                                        columns=[f\"preds_{model_name}\"],\r\n",
    "                                                                        neutralizers=riskiest_features,\r\n",
    "                                                                        proportion=0.8,\r\n",
    "                                                                        normalize=True,\r\n",
    "                                                                        era_col=ERA_COL)\r\n",
    "spinner.succeed()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<halo.halo.Halo at 0x2404faad880>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\r\n",
    "\r\n",
    "# rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\r\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\r\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\r\n",
    "validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "\r\n",
    "# get some stats about each of our models to compare...\r\n",
    "# fast_mode=True so that we skip some of the stats that are slower to calculate\r\n",
    "validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\r\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|                                        |      mean |   sharpe |\n",
      "|:---------------------------------------|----------:|---------:|\n",
      "| preds_xgboost_bare_neutral_riskiest_50 | 0.0241834 |  1.04763 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#Ensemble Lgbm + XGBoost ja neutralitzzades\r\n",
    "model_name = \"BLITTER2\"\r\n",
    "\r\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = validation_data[\"preds_model_target_neutral_riskiest_50\"] * 0.5 + validation_data[\"preds_xgboost_bare_neutral_riskiest_50\"] * 0.5\r\n",
    "tournament_data.loc[:, f\"preds_{model_name}\"] = tournament_data[\"preds_model_target_neutral_riskiest_50\"] * 0.5 + tournament_data[\"preds_xgboost_bare_neutral_riskiest_50\"] * 0.5\r\n",
    "\r\n",
    "model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\r\n",
    "\r\n",
    "validation_data[model_to_submit] = validation_data[f\"preds_{model_name}\"]\r\n",
    "tournament_data[model_to_submit] = tournament_data[f\"preds_{model_name}\"]\r\n",
    "\r\n",
    "\r\n",
    "# rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\r\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\r\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\r\n",
    "\r\n",
    "validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "\r\n",
    "validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\r\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_BLITTER2_neutral_riskiest_50 | 0.0239164 |  1.06938 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#Ensemble Lgbm + XGBoost ensemble after rank\r\n",
    "model_name = \"BLITTER3\"\r\n",
    "\r\n",
    "#ensemble on ranks\r\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = validation_data[\"preds_model_target_neutral_riskiest_50\"].rank(pct=True) + validation_data[\"preds_xgboost_bare_neutral_riskiest_50\"].rank(pct=True) / 2\r\n",
    "tournament_data.loc[:, f\"preds_{model_name}\"] = tournament_data[\"preds_model_target_neutral_riskiest_50\"].rank(pct=True) + tournament_data[\"preds_xgboost_bare_neutral_riskiest_50\"].rank(pct=True)\r\n",
    "\r\n",
    "model_to_submit = f\"preds_{model_name}\"\r\n",
    "\r\n",
    "#rank again to fix decimals coming from the /2\r\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\r\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\r\n",
    "\r\n",
    "\r\n",
    "validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "\r\n",
    "validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\r\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|                |      mean |   sharpe |\n",
      "|:---------------|----------:|---------:|\n",
      "| preds_BLITTER3 | 0.0236422 |  1.06787 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "#Blitter4 XGBRank\r\n",
    "from xgboost import XGBRanker\r\n",
    "\r\n",
    "model_name = f\"BLITTER4\"\r\n",
    "print(f\"predicting {model_name}\")\r\n",
    "model = load_model(model_name)\r\n",
    "if not model:\r\n",
    "    print(f\"model not found, training new one\")\r\n",
    "    params = {\"n_estimators\": 2000,\r\n",
    "              \"learning_rate\": 0.01,\r\n",
    "              \"max_depth\": 5,\r\n",
    "              \"num_leaves\": 2 ** 5,\r\n",
    "              \"colsample_bytree\": 0.1}\r\n",
    "\r\n",
    "                     \r\n",
    "    model = XGBRanker(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\r\n",
    "    cdf = training_data.groupby('era').agg(['count'])\r\n",
    "    group = cdf[cdf.columns[0]].values\r\n",
    "    del cdf\r\n",
    " \r\n",
    "\r\n",
    "\r\n",
    "    # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\r\n",
    "    spinner.start('Training model')\r\n",
    "    model.fit(training_data.loc[:, feature_cols], training_data[TARGET_COL], group=group)\r\n",
    "    print(f\"saving new model: {model_name}\")\r\n",
    "    save_model(model, model_name)\r\n",
    "    spinner.succeed()\r\n",
    "\r\n",
    "# check for nans and fill nans\r\n",
    "if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\r\n",
    "    cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\r\n",
    "    total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\r\n",
    "    print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\r\n",
    "    print(f\"out of {total_rows} total rows\")\r\n",
    "    print(f\"filling nans with 0.5\")\r\n",
    "    tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\r\n",
    "else:\r\n",
    "    print(\"No nans in the features this week!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicting BLITTER4\n",
      "model not found, training new one\n",
      "| Training modelsaving new model: BLITTER4\n",
      "v Training model\n",
      "No nans in the features this week!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# predict on the latest data!\r\n",
    "spinner.start('Predicting on latest data')\r\n",
    "# double check the feature that the model expects vs what is available\r\n",
    "# this prevents our pipeline from failing if Numerai adds more data and we don't have time to retrain!\r\n",
    "model_expected_features = model.get_booster().feature_names\r\n",
    "if set(model_expected_features) != set(feature_cols):\r\n",
    "    print(f\"New features are available! Might want to retrain model {model_name}.\")\r\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(validation_data.loc[:, model_expected_features])\r\n",
    "tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(tournament_data.loc[:, model_expected_features])\r\n",
    "spinner.succeed()\r\n",
    "\r\n",
    "spinner.start('Neutralizing to risky features')\r\n",
    "# getting the per era correlation of each feature vs the target\r\n",
    "all_feature_corrs = training_data.groupby(ERA_COL).apply(lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\r\n",
    "\r\n",
    "# find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\r\n",
    "riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\r\n",
    "\r\n",
    "# neutralize our predictions to the riskiest features\r\n",
    "validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=validation_data,\r\n",
    "                                                                        columns=[f\"preds_{model_name}\"],\r\n",
    "                                                                        neutralizers=riskiest_features,\r\n",
    "                                                                        proportion=0.8,\r\n",
    "                                                                        normalize=True,\r\n",
    "                                                                        era_col=ERA_COL)\r\n",
    "\r\n",
    "tournament_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=tournament_data,\r\n",
    "                                                                        columns=[f\"preds_{model_name}\"],\r\n",
    "                                                                        neutralizers=riskiest_features,\r\n",
    "                                                                        proportion=0.8,\r\n",
    "                                                                        normalize=True,\r\n",
    "                                                                        era_col=ERA_COL)\r\n",
    "spinner.succeed()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<halo.halo.Halo at 0x2404faad880>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\r\n",
    "\r\n",
    "# rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\r\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\r\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\r\n",
    "validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "\r\n",
    "# get some stats about each of our models to compare...\r\n",
    "# fast_mode=True so that we skip some of the stats that are slower to calculate\r\n",
    "validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\r\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_BLITTER4_neutral_riskiest_50 | 0.0217392 | 0.864442 |\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('numerai': conda)"
  },
  "interpreter": {
   "hash": "a85be61ea43e8b6e79189e6bca7a99912206a1ace4d1f752775c7cc0873391fd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}