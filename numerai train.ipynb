{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "from lightgbm import LGBMRegressor\r\n",
    "import gc\r\n",
    "from numerapi import NumerAPI\r\n",
    "from halo import Halo\r\n",
    "from utils import save_model, load_model, neutralize, get_biggest_change_features, validation_metrics, download_data\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "napi = NumerAPI()\r\n",
    "spinner = Halo(text='', spinner='dots')\r\n",
    "\r\n",
    "current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament\r\n",
    "print(current_round)\r\n",
    "\r\n",
    "# read in all of the new datas\r\n",
    "# tournament data and example predictions change every week so we specify the round in their names\r\n",
    "# training and validation data only change periodically, so no need to download them over again every single week\r\n",
    "napi.download_dataset(\"numerai_training_data_int8.parquet\", \"numerai_training_data_int8.parquet\")\r\n",
    "napi.download_dataset(\"numerai_tournament_data_int8.parquet\", f\"numerai_tournament_data_{current_round}_int8.parquet\")\r\n",
    "napi.download_dataset(\"numerai_validation_data_int8.parquet\", f\"numerai_validation_data_int8.parquet\")\r\n",
    "napi.download_dataset(\"example_predictions.parquet\", f\"example_predictions_{current_round}.parquet\")\r\n",
    "napi.download_dataset(\"example_validation_predictions.parquet\", \"example_validation_predictions.parquet\")\r\n",
    "\r\n",
    "spinner.start('Reading parquet data')\r\n",
    "training_data = pd.read_parquet('numerai_training_data_int8.parquet')\r\n",
    "tournament_data = pd.read_parquet(f'numerai_tournament_data_{current_round}_int8.parquet')\r\n",
    "validation_data = pd.read_parquet('numerai_validation_data_int8.parquet')\r\n",
    "example_preds = pd.read_parquet(f'example_predictions_{current_round}.parquet')\r\n",
    "validation_preds = pd.read_parquet('example_validation_predictions.parquet')\r\n",
    "spinner.succeed()\r\n",
    "\r\n",
    "EXAMPLE_PREDS_COL = \"example_preds\"\r\n",
    "validation_data[EXAMPLE_PREDS_COL] = validation_preds[\"prediction\"]\r\n",
    "\r\n",
    "TARGET_COL = \"target\"\r\n",
    "ERA_COL = \"era\"\r\n",
    "\r\n",
    "# all feature columns start with the prefix \"feature_\"\r\n",
    "#feature_cols = [c for c in training_data if c.startswith(\"feature_\")]\r\n",
    "\r\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "284\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-03 10:06:30,999 INFO numerapi.utils: target file already exists\n",
      "2021-10-03 10:06:31,001 INFO numerapi.utils: download complete\n",
      "2021-10-03 10:06:31,903 INFO numerapi.utils: target file already exists\n",
      "2021-10-03 10:06:31,905 INFO numerapi.utils: download complete\n",
      "2021-10-03 10:06:32,822 INFO numerapi.utils: target file already exists\n",
      "2021-10-03 10:06:32,823 INFO numerapi.utils: download complete\n",
      "2021-10-03 10:06:33,726 INFO numerapi.utils: target file already exists\n",
      "2021-10-03 10:06:33,728 INFO numerapi.utils: download complete\n",
      "2021-10-03 10:06:34,657 INFO numerapi.utils: target file already exists\n",
      "2021-10-03 10:06:34,659 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "v Reading parquet data\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 1
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "f=['feature_haziest_lifelike_horseback', 'feature_glare_factional_assessment', 'feature_exorbitant_myeloid_crinkle', 'feature_travelled_semipermeable_perruquier', 'feature_branched_dilatory_sunbelt', 'feature_moralistic_heartier_typhoid', 'feature_introvert_symphysial_assegai', 'feature_gullable_sanguine_incongruity', 'feature_agile_unrespited_gaucho', 'feature_canalicular_peeling_lilienthal', 'feature_unvaried_social_bangkok', 'feature_lofty_acceptable_challenge', 'feature_grandmotherly_circumnavigable_homonymity', 'feature_undivorced_unsatisfying_praetorium', 'feature_unaired_operose_lactoprotein']\r\n",
    "f+=['feature_travelled_semipermeable_perruquier', 'feature_planned_superimposed_bend', 'feature_moralistic_heartier_typhoid', 'feature_crowning_frustrate_kampala', 'feature_unaired_operose_lactoprotein', 'feature_flintier_enslaved_borsch', 'feature_cambial_bigoted_bacterioid', 'feature_jerkwater_eustatic_electrocardiograph', 'feature_unvaried_social_bangkok', 'feature_communicatory_unrecommended_velure', 'feature_lofty_acceptable_challenge', 'feature_grandmotherly_circumnavigable_homonymity', 'feature_antichristian_slangiest_idyllist', 'feature_assenting_darn_arthropod', 'feature_haziest_lifelike_horseback', 'feature_exorbitant_myeloid_crinkle', 'feature_beery_somatologic_elimination', 'feature_silver_handworked_scauper', 'feature_canalicular_peeling_lilienthal', 'feature_undivorced_unsatisfying_praetorium']\r\n",
    "f+=['feature_glare_factional_assessment', 'feature_travelled_semipermeable_perruquier', 'feature_moralistic_heartier_typhoid', 'feature_stylistic_honduran_comprador', 'feature_crowning_frustrate_kampala', 'feature_unaired_operose_lactoprotein', 'feature_flintier_enslaved_borsch', 'feature_unvaried_social_bangkok', 'feature_apomictical_motorized_vaporisation', 'feature_lofty_acceptable_challenge', 'feature_antichristian_slangiest_idyllist', 'feature_store_apteral_isocheim', 'feature_unforbidden_highbrow_kafir', 'feature_buxom_curtained_sienna', 'feature_haziest_lifelike_horseback', 'feature_exorbitant_myeloid_crinkle', 'feature_silver_handworked_scauper', 'feature_canalicular_peeling_lilienthal', 'feature_introvert_symphysial_assegai', 'feature_univalve_abdicant_distrail', 'feature_undivorced_unsatisfying_praetorium']\r\n",
    "f+=['feature_glare_factional_assessment', 'feature_unsealed_suffixal_babar', 'feature_travelled_semipermeable_perruquier', 'feature_moralistic_heartier_typhoid', 'feature_twisty_adequate_minutia', 'feature_flintier_enslaved_borsch', 'feature_slack_calefacient_tableau', 'feature_bhutan_imagism_dolerite', 'feature_unvaried_social_bangkok', 'feature_communicatory_unrecommended_velure', 'feature_lofty_acceptable_challenge', 'feature_grandmotherly_circumnavigable_homonymity', 'feature_chuffier_analectic_conchiolin', 'feature_antichristian_slangiest_idyllist', 'feature_unwonted_trusted_fixative', 'feature_haziest_lifelike_horseback', 'feature_exorbitant_myeloid_crinkle', 'feature_beery_somatologic_elimination', 'feature_winsome_irreproachable_milkfish', 'feature_gullable_sanguine_incongruity', 'feature_silver_handworked_scauper', 'feature_canalicular_peeling_lilienthal', 'feature_introvert_symphysial_assegai', 'feature_undivorced_unsatisfying_praetorium']\r\n",
    "\r\n",
    "feature_cols = list(set(f))\r\n",
    "len(feature_cols)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "metadata": {},
     "execution_count": 2
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def create_lgbm_regressor(model_name:str):\r\n",
    "    model = load_model(model_name)\r\n",
    "    if not model:\r\n",
    "        print(f\"model not found, training new one\")\r\n",
    "        params = {\"n_estimators\": 2000,\r\n",
    "                \"learning_rate\": 0.01,\r\n",
    "                \"max_depth\": 5,\r\n",
    "                \"num_leaves\": 2 ** 5,\r\n",
    "                \"colsample_bytree\": 0.1}\r\n",
    "\r\n",
    "        model = LGBMRegressor(**params)\r\n",
    "\r\n",
    "        # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\r\n",
    "        spinner.start('Training model')\r\n",
    "        model.fit(training_data.loc[:, feature_cols], training_data[TARGET_COL])\r\n",
    "        print(f\"saving new model: {model_name}\")\r\n",
    "        save_model(model, model_name)\r\n",
    "        spinner.succeed()\r\n",
    "\r\n",
    "    # check for nans and fill nans\r\n",
    "    if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\r\n",
    "        cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\r\n",
    "        total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\r\n",
    "        print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\r\n",
    "        print(f\"out of {total_rows} total rows\")\r\n",
    "        print(f\"filling nans with 0.5\")\r\n",
    "        tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\r\n",
    "    else:\r\n",
    "        print(\"No nans in the features this week!\")\r\n",
    "\r\n",
    "    return model\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def save_prediction(model, model_name, neutralize_proportion = 0.8, is_xgbRanker = False, use_pool = False):\r\n",
    "    spinner.start('Predicting on latest data')\r\n",
    "    # double check the feature that the model expects vs what is available\r\n",
    "    # this prevents our pipeline from failing if Numerai adds more data and we don't have time to retrain!\r\n",
    "\r\n",
    "\r\n",
    "    try:\r\n",
    "        model_expected_features = model.booster_.feature_name()\r\n",
    "    except:\r\n",
    "        try:\r\n",
    "            model_expected_features = model.get_booster().feature_names\r\n",
    "        except:\r\n",
    "            model_expected_features = model.feature_names_\r\n",
    "        \r\n",
    "    if set(model_expected_features) != set(feature_cols):\r\n",
    "        print(f\"New features are available! Might want to retrain model {model_name}.\")\r\n",
    "\r\n",
    "    if not use_pool:\r\n",
    "        # if is_xgbRanker:\r\n",
    "            \r\n",
    "        #     for era in validation_data.era.unique():\r\n",
    "        #         print(f\"predicting valid era:{era}\")\r\n",
    "        #         validation_data.loc[validation_data.era == era, f\"preds_{model_name}\"] = \\\r\n",
    "        #             model.predict(validation_data.loc[validation_data.era == era, model_expected_features])\r\n",
    "\r\n",
    "        #     for era in tournament_data.era.unique():\r\n",
    "        #         print(f\"predicting tournament era:{era}\")\r\n",
    "        #         tournament_data.loc[tournament_data.era == era, f\"preds_{model_name}\"] = \\\r\n",
    "        #             model.predict(tournament_data.loc[tournament_data.era == era, model_expected_features])\r\n",
    "        # else:\r\n",
    "        validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(validation_data.loc[:, model_expected_features])\r\n",
    "        tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(tournament_data.loc[:, model_expected_features])\r\n",
    "    else:\r\n",
    "        #catboost ranker is not compayible with sklearn\r\n",
    "\r\n",
    "        test_pool = Pool(\r\n",
    "            data=validation_data.loc[:, feature_cols],\r\n",
    "            label=validation_data[TARGET_COL],\r\n",
    "            group_id=validation_data.era\r\n",
    "        )\r\n",
    "        validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(test_pool)\r\n",
    "\r\n",
    "        tournament_pool = Pool (\r\n",
    "            data = tournament_data.loc[:, model_expected_features],\r\n",
    "            group_id = tournament_data.era\r\n",
    "        )\r\n",
    "\r\n",
    "        tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(tournament_pool)\r\n",
    "\r\n",
    "    spinner.succeed()\r\n",
    "\r\n",
    "    spinner.start('Neutralizing to risky features')\r\n",
    "    # getting the per era correlation of each feature vs the target\r\n",
    "    all_feature_corrs = training_data.groupby(ERA_COL).apply(lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\r\n",
    "\r\n",
    "    # find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\r\n",
    "    riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\r\n",
    "\r\n",
    "    # neutralize our predictions to the riskiest features\r\n",
    "    validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=validation_data,\r\n",
    "                                                                            columns=[f\"preds_{model_name}\"],\r\n",
    "                                                                            neutralizers=riskiest_features,\r\n",
    "                                                                            proportion=neutralize_proportion,\r\n",
    "                                                                            normalize=True,\r\n",
    "                                                                            era_col=ERA_COL)\r\n",
    "\r\n",
    "    tournament_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=tournament_data,\r\n",
    "                                                                            columns=[f\"preds_{model_name}\"],\r\n",
    "                                                                            neutralizers=riskiest_features,\r\n",
    "                                                                            proportion=neutralize_proportion,\r\n",
    "                                                                            normalize=True,\r\n",
    "                                                                            era_col=ERA_COL)\r\n",
    "    spinner.succeed()\r\n",
    "\r\n",
    "    model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\r\n",
    "\r\n",
    "    # rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\r\n",
    "    validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\r\n",
    "    tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\r\n",
    "    validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "    tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "\r\n",
    "    # get some stats about each of our models to compare...\r\n",
    "    # fast_mode=True so that we skip some of the stats that are slower to calculate\r\n",
    "    validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\r\n",
    "    print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Blitter4 XGBRank\r\n",
    "from xgboost import XGBRanker\r\n",
    "from collections import Counter\r\n",
    "\r\n",
    "\r\n",
    "def create_xgb_ranker(model_name:str):\r\n",
    "    model = load_model(model_name)\r\n",
    "    if not model:\r\n",
    "        print(f\"model not found, training new one\")\r\n",
    "\r\n",
    "                        \r\n",
    "        model = XGBRanker(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\r\n",
    "        # cdf = training_data.groupby('era').agg(['count'])\r\n",
    "        # group = cdf[cdf.columns[0]].values\r\n",
    "        # del cdf\r\n",
    "        group = Counter(training_data.era).values()\r\n",
    "\r\n",
    "        # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\r\n",
    "        spinner.start('Training model')\r\n",
    "        model.fit(training_data.loc[:, feature_cols], training_data[TARGET_COL], group=group)\r\n",
    "        print(f\"saving new model: {model_name}\")\r\n",
    "        save_model(model, model_name)\r\n",
    "        spinner.succeed()\r\n",
    "\r\n",
    "    # check for nans and fill nans\r\n",
    "    if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\r\n",
    "        cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\r\n",
    "        total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\r\n",
    "        print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\r\n",
    "        print(f\"out of {total_rows} total rows\")\r\n",
    "        print(f\"filling nans with 0.5\")\r\n",
    "        tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\r\n",
    "    else:\r\n",
    "        print(\"No nans in the features this week!\")\r\n",
    "\r\n",
    "    return model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from xgboost import XGBRegressor\r\n",
    "\r\n",
    "def create_xgb_regressor(model_name:str):\r\n",
    "    model = load_model(model_name)\r\n",
    "    if not model:\r\n",
    "        print(f\"model not found, training new one\")\r\n",
    "        params = {\"n_estimators\": 2000,\r\n",
    "                \"learning_rate\": 0.01,\r\n",
    "                \"max_depth\": 5,\r\n",
    "                \"num_leaves\": 2 ** 5,\r\n",
    "                \"colsample_bytree\": 0.1}\r\n",
    "\r\n",
    "        model = XGBRegressor(max_depth=5, learning_rate=0.01, \\\r\n",
    "                        n_estimators=2000, colsample_bytree=0.1) #provar regularitzacions\r\n",
    "\r\n",
    "        # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\r\n",
    "        spinner.start('Training model')\r\n",
    "        model.fit(training_data.loc[:, feature_cols], training_data[TARGET_COL])\r\n",
    "        print(f\"saving new model: {model_name}\")\r\n",
    "        save_model(model, model_name)\r\n",
    "        spinner.succeed()\r\n",
    "\r\n",
    "    # check for nans and fill nans\r\n",
    "    if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\r\n",
    "        cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\r\n",
    "        total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\r\n",
    "        print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\r\n",
    "        print(f\"out of {total_rows} total rows\")\r\n",
    "        print(f\"filling nans with 0.5\")\r\n",
    "        tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\r\n",
    "    else:\r\n",
    "        print(\"No nans in the features this week!\")\r\n",
    "\r\n",
    "    return model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#BLITTER5 - Catboost regressor\r\n",
    "from catboost import CatBoostRegressor\r\n",
    "\r\n",
    "def create_catboost_regressor(model_name:str):\r\n",
    "\r\n",
    "    model = load_model(model_name)\r\n",
    "    if not model:\r\n",
    "        print(f\"model not found, training new one\")\r\n",
    "\r\n",
    "        model = CatBoostRegressor(max_depth=5, learning_rate=0.01, \\\r\n",
    "                        n_estimators=2000, rsm=0.1) #provar regularitzacions\r\n",
    "\r\n",
    "        # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\r\n",
    "        spinner.start('Training model')\r\n",
    "        model.fit(training_data.loc[:, feature_cols], training_data[TARGET_COL])\r\n",
    "        print(f\"saving new model: {model_name}\")\r\n",
    "        save_model(model, model_name)\r\n",
    "        spinner.succeed()\r\n",
    "\r\n",
    "    # check for nans and fill nans\r\n",
    "    if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\r\n",
    "        cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\r\n",
    "        total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\r\n",
    "        print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\r\n",
    "        print(f\"out of {total_rows} total rows\")\r\n",
    "        print(f\"filling nans with 0.5\")\r\n",
    "        tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\r\n",
    "    else:\r\n",
    "        print(\"No nans in the features this week!\")\r\n",
    "    return model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from catboost import CatBoostRanker, Pool\r\n",
    "\r\n",
    "def create_catboost_ranker(model_name : str, loss_function : str, plot = True, verbose = False):\r\n",
    "    gc.collect()\r\n",
    "    print(f\"predicting {model_name}\")\r\n",
    "    model = load_model(model_name)\r\n",
    "    if not model:\r\n",
    "        print(f\"model not found, training new one\")\r\n",
    "        parameters = {\r\n",
    "            'iterations': 200,\r\n",
    "            'custom_metric': ['NDCG', 'PFound'],\r\n",
    "            'verbose': verbose,\r\n",
    "            'random_seed': 0,\r\n",
    "            'rsm' : 0.1,\r\n",
    "            'learning_rate' : 0.01,\r\n",
    "            'loss_function' : loss_function,\r\n",
    "            #'train_dir' : loss_function\r\n",
    "        }\r\n",
    "\r\n",
    "        train_pool = Pool(\r\n",
    "            data=training_data.loc[:, feature_cols],\r\n",
    "            label=training_data[TARGET_COL],\r\n",
    "            group_id=training_data.era\r\n",
    "        )\r\n",
    "\r\n",
    "        test_pool = Pool(\r\n",
    "            data=validation_data.loc[:, feature_cols],\r\n",
    "            label=validation_data[TARGET_COL],\r\n",
    "            group_id=validation_data.era\r\n",
    "        )\r\n",
    "\r\n",
    "        model = CatBoostRanker(**parameters)\r\n",
    "\r\n",
    "\r\n",
    "        # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\r\n",
    "        #spinner.start('Training model')\r\n",
    "        model.fit(train_pool, eval_set=test_pool, early_stopping_rounds=10, plot=plot)\r\n",
    "\r\n",
    "        print(f\"saving new model: {model_name}\")\r\n",
    "        save_model(model, model_name)\r\n",
    "        #spinner.succeed()\r\n",
    "\r\n",
    "    # check for nans and fill nans\r\n",
    "    if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\r\n",
    "        cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\r\n",
    "        total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\r\n",
    "        print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\r\n",
    "        print(f\"out of {total_rows} total rows\")\r\n",
    "        print(f\"filling nans with 0.5\")\r\n",
    "        tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\r\n",
    "    else:\r\n",
    "        print(\"No nans in the features this week!\")\r\n",
    "\r\n",
    "    return model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "model_name = f\"blitter\"\r\n",
    "model = create_lgbm_regressor(model_name)\r\n",
    "save_prediction(model, model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicting blitter\n",
      "No nans in the features this week!\n",
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                   |      mean |   sharpe |\n",
      "|:----------------------------------|----------:|---------:|\n",
      "| preds_blitter_neutral_riskiest_50 | 0.0228259 |  1.05479 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "model_name = \"blitter1\" #f\"xgboost_bare\"\r\n",
    "model = create_xgb_regressor(model_name)\r\n",
    "save_prediction(model, model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No nans in the features this week!\n",
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_blitter1_neutral_riskiest_50 | 0.0241834 |  1.04763 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "#Ensemble Lgbm + XGBoost ja neutralitzzades\r\n",
    "model_name = \"BLITTER2\"\r\n",
    "\r\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = validation_data[\"preds_blitter_neutral_riskiest_50\"] * 0.5 + validation_data[\"preds_blitter1_neutral_riskiest_50\"] * 0.5\r\n",
    "tournament_data.loc[:, f\"preds_{model_name}\"] = tournament_data[\"preds_blitter_neutral_riskiest_50\"] * 0.5 + tournament_data[\"preds_blitter1_neutral_riskiest_50\"] * 0.5\r\n",
    "\r\n",
    "model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\r\n",
    "\r\n",
    "validation_data[model_to_submit] = validation_data[f\"preds_{model_name}\"]\r\n",
    "tournament_data[model_to_submit] = tournament_data[f\"preds_{model_name}\"]\r\n",
    "\r\n",
    "\r\n",
    "# rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\r\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\r\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\r\n",
    "\r\n",
    "validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "\r\n",
    "validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\r\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_BLITTER2_neutral_riskiest_50 | 0.0239164 |  1.06938 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "#Ensemble Lgbm + XGBoost ensemble after rank\r\n",
    "model_name = \"BLITTER3\"\r\n",
    "\r\n",
    "#ensemble on ranks\r\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = validation_data[\"preds_blitter_neutral_riskiest_50\"].rank(pct=True) + validation_data[\"preds_blitter1_neutral_riskiest_50\"].rank(pct=True) / 2\r\n",
    "tournament_data.loc[:, f\"preds_{model_name}\"] = tournament_data[\"preds_blitter_neutral_riskiest_50\"].rank(pct=True) + tournament_data[\"preds_blitter1_neutral_riskiest_50\"].rank(pct=True) / 2\r\n",
    "\r\n",
    "model_to_submit = f\"preds_{model_name}\"\r\n",
    "\r\n",
    "#rank again to fix decimals coming from the /2\r\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\r\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\r\n",
    "\r\n",
    "\r\n",
    "validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}_{model_to_submit}.csv\")\r\n",
    "\r\n",
    "validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\r\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|                |      mean |   sharpe |\n",
      "|:---------------|----------:|---------:|\n",
      "| preds_BLITTER3 | 0.0236422 |  1.06787 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "model_name = f\"BLITTER4\"\r\n",
    "model = create_xgb_ranker(model_name)\r\n",
    "save_prediction(model, model_name, is_xgbRanker= True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No nans in the features this week!\n",
      "\\ Predicting on latest datapredicting valid era:0857\n",
      "| Predicting on latest datapredicting valid era:0858\n",
      "/ Predicting on latest datapredicting valid era:0859\n",
      "- Predicting on latest datapredicting valid era:0860\n",
      "\\ Predicting on latest datapredicting valid era:0861\n",
      "| Predicting on latest datapredicting valid era:0862\n",
      "/ Predicting on latest datapredicting valid era:0863\n",
      "- Predicting on latest datapredicting valid era:0864\n",
      "\\ Predicting on latest datapredicting valid era:0865\n",
      "| Predicting on latest datapredicting valid era:0866\n",
      "/ Predicting on latest datapredicting valid era:0867\n",
      "- Predicting on latest datapredicting valid era:0868\n",
      "\\ Predicting on latest datapredicting valid era:0869\n",
      "| Predicting on latest datapredicting valid era:0870\n",
      "/ Predicting on latest datapredicting valid era:0871\n",
      "- Predicting on latest datapredicting valid era:0872\n",
      "\\ Predicting on latest datapredicting valid era:0873\n",
      "| Predicting on latest datapredicting valid era:0874\n",
      "/ Predicting on latest datapredicting valid era:0875\n",
      "- Predicting on latest datapredicting valid era:0876\n",
      "\\ Predicting on latest datapredicting valid era:0877\n",
      "| Predicting on latest datapredicting valid era:0878\n",
      "/ Predicting on latest datapredicting valid era:0879\n",
      "- Predicting on latest datapredicting valid era:0880\n",
      "\\ Predicting on latest datapredicting valid era:0881\n",
      "| Predicting on latest datapredicting valid era:0882\n",
      "/ Predicting on latest datapredicting valid era:0883\n",
      "- Predicting on latest datapredicting valid era:0884\n",
      "\\ Predicting on latest datapredicting valid era:0885\n",
      "| Predicting on latest datapredicting valid era:0886\n",
      "/ Predicting on latest datapredicting valid era:0887\n",
      "- Predicting on latest datapredicting valid era:0888\n",
      "\\ Predicting on latest datapredicting valid era:0889\n",
      "| Predicting on latest datapredicting valid era:0890\n",
      "/ Predicting on latest datapredicting valid era:0891\n",
      "- Predicting on latest datapredicting valid era:0892\n",
      "\\ Predicting on latest datapredicting valid era:0893\n",
      "| Predicting on latest datapredicting valid era:0894\n",
      "/ Predicting on latest datapredicting valid era:0895\n",
      "- Predicting on latest datapredicting valid era:0896\n",
      "\\ Predicting on latest datapredicting valid era:0897\n",
      "/ Predicting on latest datapredicting valid era:0898\n",
      "- Predicting on latest datapredicting valid era:0899\n",
      "\\ Predicting on latest datapredicting valid era:0900\n",
      "| Predicting on latest datapredicting valid era:0901\n",
      "/ Predicting on latest datapredicting valid era:0902\n",
      "- Predicting on latest datapredicting valid era:0903\n",
      "\\ Predicting on latest datapredicting valid era:0904\n",
      "| Predicting on latest datapredicting valid era:0905\n",
      "/ Predicting on latest datapredicting valid era:0906\n",
      "- Predicting on latest datapredicting valid era:0907\n",
      "\\ Predicting on latest datapredicting valid era:0908\n",
      "| Predicting on latest datapredicting valid era:0909\n",
      "/ Predicting on latest datapredicting valid era:0910\n",
      "- Predicting on latest datapredicting valid era:0911\n",
      "\\ Predicting on latest datapredicting valid era:0912\n",
      "| Predicting on latest datapredicting valid era:0913\n",
      "/ Predicting on latest datapredicting valid era:0914\n",
      "- Predicting on latest datapredicting valid era:0915\n",
      "\\ Predicting on latest datapredicting valid era:0916\n",
      "| Predicting on latest datapredicting valid era:0917\n",
      "/ Predicting on latest datapredicting valid era:0918\n",
      "- Predicting on latest datapredicting valid era:0919\n",
      "\\ Predicting on latest datapredicting valid era:0920\n",
      "| Predicting on latest datapredicting valid era:0921\n",
      "/ Predicting on latest datapredicting valid era:0922\n",
      "- Predicting on latest datapredicting valid era:0923\n",
      "\\ Predicting on latest datapredicting valid era:0924\n",
      "| Predicting on latest datapredicting valid era:0925\n",
      "/ Predicting on latest datapredicting valid era:0926\n",
      "- Predicting on latest datapredicting valid era:0927\n",
      "\\ Predicting on latest datapredicting valid era:0928\n",
      "| Predicting on latest datapredicting valid era:0929\n",
      "/ Predicting on latest datapredicting valid era:0930\n",
      "- Predicting on latest datapredicting valid era:0931\n",
      "\\ Predicting on latest datapredicting valid era:0932\n",
      "| Predicting on latest datapredicting valid era:0933\n",
      "/ Predicting on latest datapredicting valid era:0934\n",
      "- Predicting on latest datapredicting valid era:0935\n",
      "\\ Predicting on latest datapredicting valid era:0936\n",
      "| Predicting on latest datapredicting valid era:0937\n",
      "/ Predicting on latest datapredicting valid era:0938\n",
      "- Predicting on latest datapredicting valid era:0939\n",
      "\\ Predicting on latest datapredicting valid era:0940\n",
      "| Predicting on latest datapredicting valid era:0941\n",
      "/ Predicting on latest datapredicting valid era:0942\n",
      "- Predicting on latest datapredicting valid era:0943\n",
      "\\ Predicting on latest datapredicting valid era:0944\n",
      "| Predicting on latest datapredicting valid era:0945\n",
      "/ Predicting on latest datapredicting valid era:0946\n",
      "- Predicting on latest datapredicting valid era:0947\n",
      "\\ Predicting on latest datapredicting valid era:0948\n",
      "| Predicting on latest datapredicting valid era:0949\n",
      "/ Predicting on latest datapredicting valid era:0950\n",
      "- Predicting on latest datapredicting valid era:0951\n",
      "\\ Predicting on latest datapredicting valid era:0952\n",
      "| Predicting on latest datapredicting valid era:0953\n",
      "/ Predicting on latest datapredicting valid era:0954\n",
      "- Predicting on latest datapredicting valid era:0955\n",
      "\\ Predicting on latest datapredicting valid era:0956\n",
      "| Predicting on latest datapredicting valid era:0957\n",
      "/ Predicting on latest datapredicting valid era:0958\n",
      "- Predicting on latest datapredicting valid era:0959\n",
      "\\ Predicting on latest datapredicting valid era:0960\n",
      "| Predicting on latest datapredicting valid era:0961\n",
      "/ Predicting on latest datapredicting tournament era:0575\n",
      "\\ Predicting on latest datapredicting tournament era:0576\n",
      "/ Predicting on latest datapredicting tournament era:0577\n",
      "\\ Predicting on latest datapredicting tournament era:0578\n",
      "/ Predicting on latest datapredicting tournament era:0579\n",
      "\\ Predicting on latest datapredicting tournament era:0580\n",
      "/ Predicting on latest datapredicting tournament era:0581\n",
      "\\ Predicting on latest datapredicting tournament era:0582\n",
      "/ Predicting on latest datapredicting tournament era:0583\n",
      "\\ Predicting on latest datapredicting tournament era:0584\n",
      "/ Predicting on latest datapredicting tournament era:0585\n",
      "\\ Predicting on latest datapredicting tournament era:0586\n",
      "/ Predicting on latest datapredicting tournament era:0587\n",
      "\\ Predicting on latest datapredicting tournament era:0588\n",
      "/ Predicting on latest datapredicting tournament era:0589\n",
      "\\ Predicting on latest datapredicting tournament era:0590\n",
      "/ Predicting on latest datapredicting tournament era:0591\n",
      "\\ Predicting on latest datapredicting tournament era:0592\n",
      "/ Predicting on latest datapredicting tournament era:0593\n",
      "\\ Predicting on latest datapredicting tournament era:0594\n",
      "/ Predicting on latest datapredicting tournament era:0595\n",
      "\\ Predicting on latest datapredicting tournament era:0596\n",
      "/ Predicting on latest datapredicting tournament era:0597\n",
      "\\ Predicting on latest datapredicting tournament era:0598\n",
      "/ Predicting on latest datapredicting tournament era:0599\n",
      "\\ Predicting on latest datapredicting tournament era:0600\n",
      "/ Predicting on latest datapredicting tournament era:0601\n",
      "\\ Predicting on latest datapredicting tournament era:0602\n",
      "/ Predicting on latest datapredicting tournament era:0603\n",
      "\\ Predicting on latest datapredicting tournament era:0604\n",
      "/ Predicting on latest datapredicting tournament era:0605\n",
      "\\ Predicting on latest datapredicting tournament era:0606\n",
      "/ Predicting on latest datapredicting tournament era:0607\n",
      "\\ Predicting on latest datapredicting tournament era:0608\n",
      "/ Predicting on latest datapredicting tournament era:0609\n",
      "\\ Predicting on latest datapredicting tournament era:0610\n",
      "/ Predicting on latest datapredicting tournament era:0611\n",
      "\\ Predicting on latest datapredicting tournament era:0612\n",
      "/ Predicting on latest datapredicting tournament era:0613\n",
      "\\ Predicting on latest datapredicting tournament era:0614\n",
      "/ Predicting on latest datapredicting tournament era:0615\n",
      "\\ Predicting on latest datapredicting tournament era:0616\n",
      "/ Predicting on latest datapredicting tournament era:0617\n",
      "\\ Predicting on latest datapredicting tournament era:0618\n",
      "/ Predicting on latest datapredicting tournament era:0619\n",
      "\\ Predicting on latest datapredicting tournament era:0620\n",
      "/ Predicting on latest datapredicting tournament era:0621\n",
      "\\ Predicting on latest datapredicting tournament era:0622\n",
      "/ Predicting on latest datapredicting tournament era:0623\n",
      "\\ Predicting on latest datapredicting tournament era:0624\n",
      "/ Predicting on latest datapredicting tournament era:0625\n",
      "\\ Predicting on latest datapredicting tournament era:0626\n",
      "/ Predicting on latest datapredicting tournament era:0627\n",
      "\\ Predicting on latest datapredicting tournament era:0628\n",
      "/ Predicting on latest datapredicting tournament era:0629\n",
      "\\ Predicting on latest datapredicting tournament era:0630\n",
      "/ Predicting on latest datapredicting tournament era:0631\n",
      "\\ Predicting on latest datapredicting tournament era:0632\n",
      "/ Predicting on latest datapredicting tournament era:0633\n",
      "\\ Predicting on latest datapredicting tournament era:0634\n",
      "/ Predicting on latest datapredicting tournament era:0635\n",
      "\\ Predicting on latest datapredicting tournament era:0636\n",
      "/ Predicting on latest datapredicting tournament era:0637\n",
      "\\ Predicting on latest datapredicting tournament era:0638\n",
      "/ Predicting on latest datapredicting tournament era:0639\n",
      "\\ Predicting on latest datapredicting tournament era:0640\n",
      "/ Predicting on latest datapredicting tournament era:0641\n",
      "\\ Predicting on latest datapredicting tournament era:0642\n",
      "/ Predicting on latest datapredicting tournament era:0643\n",
      "\\ Predicting on latest datapredicting tournament era:0644\n",
      "/ Predicting on latest datapredicting tournament era:0645\n",
      "\\ Predicting on latest datapredicting tournament era:0646\n",
      "/ Predicting on latest datapredicting tournament era:0647\n",
      "\\ Predicting on latest datapredicting tournament era:0648\n",
      "/ Predicting on latest datapredicting tournament era:0649\n",
      "\\ Predicting on latest datapredicting tournament era:0650\n",
      "/ Predicting on latest datapredicting tournament era:0651\n",
      "\\ Predicting on latest datapredicting tournament era:0652\n",
      "/ Predicting on latest datapredicting tournament era:0653\n",
      "\\ Predicting on latest datapredicting tournament era:0654\n",
      "/ Predicting on latest datapredicting tournament era:0655\n",
      "\\ Predicting on latest datapredicting tournament era:0656\n",
      "/ Predicting on latest datapredicting tournament era:0657\n",
      "\\ Predicting on latest datapredicting tournament era:0658\n",
      "/ Predicting on latest datapredicting tournament era:0659\n",
      "\\ Predicting on latest datapredicting tournament era:0660\n",
      "/ Predicting on latest datapredicting tournament era:0661\n",
      "\\ Predicting on latest datapredicting tournament era:0662\n",
      "/ Predicting on latest datapredicting tournament era:0663\n",
      "\\ Predicting on latest datapredicting tournament era:0664\n",
      "/ Predicting on latest datapredicting tournament era:0665\n",
      "\\ Predicting on latest datapredicting tournament era:0666\n",
      "/ Predicting on latest datapredicting tournament era:0667\n",
      "\\ Predicting on latest datapredicting tournament era:0668\n",
      "/ Predicting on latest datapredicting tournament era:0669\n",
      "\\ Predicting on latest datapredicting tournament era:0670\n",
      "/ Predicting on latest datapredicting tournament era:0671\n",
      "\\ Predicting on latest datapredicting tournament era:0672\n",
      "/ Predicting on latest datapredicting tournament era:0673\n",
      "\\ Predicting on latest datapredicting tournament era:0674\n",
      "/ Predicting on latest datapredicting tournament era:0675\n",
      "\\ Predicting on latest datapredicting tournament era:0676\n",
      "/ Predicting on latest datapredicting tournament era:0677\n",
      "\\ Predicting on latest datapredicting tournament era:0678\n",
      "/ Predicting on latest datapredicting tournament era:0679\n",
      "\\ Predicting on latest datapredicting tournament era:0680\n",
      "/ Predicting on latest datapredicting tournament era:0681\n",
      "\\ Predicting on latest datapredicting tournament era:0682\n",
      "/ Predicting on latest datapredicting tournament era:0683\n",
      "\\ Predicting on latest datapredicting tournament era:0684\n",
      "/ Predicting on latest datapredicting tournament era:0685\n",
      "\\ Predicting on latest datapredicting tournament era:0686\n",
      "/ Predicting on latest datapredicting tournament era:0687\n",
      "\\ Predicting on latest datapredicting tournament era:0688\n",
      "/ Predicting on latest datapredicting tournament era:0689\n",
      "\\ Predicting on latest datapredicting tournament era:0690\n",
      "/ Predicting on latest datapredicting tournament era:0691\n",
      "\\ Predicting on latest datapredicting tournament era:0692\n",
      "/ Predicting on latest datapredicting tournament era:0693\n",
      "\\ Predicting on latest datapredicting tournament era:0694\n",
      "/ Predicting on latest datapredicting tournament era:0695\n",
      "\\ Predicting on latest datapredicting tournament era:0696\n",
      "/ Predicting on latest datapredicting tournament era:0697\n",
      "\\ Predicting on latest datapredicting tournament era:0698\n",
      "/ Predicting on latest datapredicting tournament era:0699\n",
      "\\ Predicting on latest datapredicting tournament era:0700\n",
      "/ Predicting on latest datapredicting tournament era:0701\n",
      "\\ Predicting on latest datapredicting tournament era:0702\n",
      "/ Predicting on latest datapredicting tournament era:0703\n",
      "\\ Predicting on latest datapredicting tournament era:0704\n",
      "/ Predicting on latest datapredicting tournament era:0705\n",
      "\\ Predicting on latest datapredicting tournament era:0706\n",
      "/ Predicting on latest datapredicting tournament era:0707\n",
      "\\ Predicting on latest datapredicting tournament era:0708\n",
      "/ Predicting on latest datapredicting tournament era:0709\n",
      "\\ Predicting on latest datapredicting tournament era:0710\n",
      "/ Predicting on latest datapredicting tournament era:0711\n",
      "\\ Predicting on latest datapredicting tournament era:0712\n",
      "/ Predicting on latest datapredicting tournament era:0713\n",
      "\\ Predicting on latest datapredicting tournament era:0714\n",
      "/ Predicting on latest datapredicting tournament era:0715\n",
      "\\ Predicting on latest datapredicting tournament era:0716\n",
      "/ Predicting on latest datapredicting tournament era:0717\n",
      "\\ Predicting on latest datapredicting tournament era:0718\n",
      "/ Predicting on latest datapredicting tournament era:0719\n",
      "\\ Predicting on latest datapredicting tournament era:0720\n",
      "/ Predicting on latest datapredicting tournament era:0721\n",
      "\\ Predicting on latest datapredicting tournament era:0722\n",
      "/ Predicting on latest datapredicting tournament era:0723\n",
      "\\ Predicting on latest datapredicting tournament era:0724\n",
      "/ Predicting on latest datapredicting tournament era:0725\n",
      "\\ Predicting on latest datapredicting tournament era:0726\n",
      "/ Predicting on latest datapredicting tournament era:0727\n",
      "\\ Predicting on latest datapredicting tournament era:0728\n",
      "/ Predicting on latest datapredicting tournament era:0729\n",
      "\\ Predicting on latest datapredicting tournament era:0730\n",
      "/ Predicting on latest datapredicting tournament era:0731\n",
      "\\ Predicting on latest datapredicting tournament era:0732\n",
      "/ Predicting on latest datapredicting tournament era:0733\n",
      "\\ Predicting on latest datapredicting tournament era:0734\n",
      "/ Predicting on latest datapredicting tournament era:0735\n",
      "\\ Predicting on latest datapredicting tournament era:0736\n",
      "/ Predicting on latest datapredicting tournament era:0737\n",
      "\\ Predicting on latest datapredicting tournament era:0738\n",
      "/ Predicting on latest datapredicting tournament era:0739\n",
      "\\ Predicting on latest datapredicting tournament era:0740\n",
      "/ Predicting on latest datapredicting tournament era:0741\n",
      "\\ Predicting on latest datapredicting tournament era:0742\n",
      "/ Predicting on latest datapredicting tournament era:0743\n",
      "\\ Predicting on latest datapredicting tournament era:0744\n",
      "/ Predicting on latest datapredicting tournament era:0745\n",
      "\\ Predicting on latest datapredicting tournament era:0746\n",
      "/ Predicting on latest datapredicting tournament era:0747\n",
      "\\ Predicting on latest datapredicting tournament era:0748\n",
      "/ Predicting on latest datapredicting tournament era:0749\n",
      "\\ Predicting on latest datapredicting tournament era:0750\n",
      "/ Predicting on latest datapredicting tournament era:0751\n",
      "\\ Predicting on latest datapredicting tournament era:0752\n",
      "/ Predicting on latest datapredicting tournament era:0753\n",
      "\\ Predicting on latest datapredicting tournament era:0754\n",
      "/ Predicting on latest datapredicting tournament era:0755\n",
      "\\ Predicting on latest datapredicting tournament era:0756\n",
      "/ Predicting on latest datapredicting tournament era:0757\n",
      "\\ Predicting on latest datapredicting tournament era:0758\n",
      "/ Predicting on latest datapredicting tournament era:0759\n",
      "\\ Predicting on latest datapredicting tournament era:0760\n",
      "/ Predicting on latest datapredicting tournament era:0761\n",
      "\\ Predicting on latest datapredicting tournament era:0762\n",
      "/ Predicting on latest datapredicting tournament era:0763\n",
      "\\ Predicting on latest datapredicting tournament era:0764\n",
      "/ Predicting on latest datapredicting tournament era:0765\n",
      "\\ Predicting on latest datapredicting tournament era:0766\n",
      "/ Predicting on latest datapredicting tournament era:0767\n",
      "\\ Predicting on latest datapredicting tournament era:0768\n",
      "/ Predicting on latest datapredicting tournament era:0769\n",
      "\\ Predicting on latest datapredicting tournament era:0770\n",
      "/ Predicting on latest datapredicting tournament era:0771\n",
      "\\ Predicting on latest datapredicting tournament era:0772\n",
      "/ Predicting on latest datapredicting tournament era:0773\n",
      "\\ Predicting on latest datapredicting tournament era:0774\n",
      "/ Predicting on latest datapredicting tournament era:0775\n",
      "\\ Predicting on latest datapredicting tournament era:0776\n",
      "/ Predicting on latest datapredicting tournament era:0777\n",
      "\\ Predicting on latest datapredicting tournament era:0778\n",
      "/ Predicting on latest datapredicting tournament era:0779\n",
      "\\ Predicting on latest datapredicting tournament era:0780\n",
      "/ Predicting on latest datapredicting tournament era:0781\n",
      "\\ Predicting on latest datapredicting tournament era:0782\n",
      "/ Predicting on latest datapredicting tournament era:0783\n",
      "\\ Predicting on latest datapredicting tournament era:0784\n",
      "/ Predicting on latest datapredicting tournament era:0785\n",
      "\\ Predicting on latest datapredicting tournament era:0786\n",
      "/ Predicting on latest datapredicting tournament era:0787\n",
      "\\ Predicting on latest datapredicting tournament era:0788\n",
      "/ Predicting on latest datapredicting tournament era:0789\n",
      "\\ Predicting on latest datapredicting tournament era:0790\n",
      "/ Predicting on latest datapredicting tournament era:0791\n",
      "\\ Predicting on latest datapredicting tournament era:0792\n",
      "/ Predicting on latest datapredicting tournament era:0793\n",
      "\\ Predicting on latest datapredicting tournament era:0794\n",
      "/ Predicting on latest datapredicting tournament era:0795\n",
      "\\ Predicting on latest datapredicting tournament era:0796\n",
      "/ Predicting on latest datapredicting tournament era:0797\n",
      "\\ Predicting on latest datapredicting tournament era:0798\n",
      "/ Predicting on latest datapredicting tournament era:0799\n",
      "\\ Predicting on latest datapredicting tournament era:0800\n",
      "/ Predicting on latest datapredicting tournament era:0801\n",
      "\\ Predicting on latest datapredicting tournament era:0802\n",
      "/ Predicting on latest datapredicting tournament era:0803\n",
      "\\ Predicting on latest datapredicting tournament era:0804\n",
      "/ Predicting on latest datapredicting tournament era:0805\n",
      "\\ Predicting on latest datapredicting tournament era:0806\n",
      "/ Predicting on latest datapredicting tournament era:0807\n",
      "\\ Predicting on latest datapredicting tournament era:0808\n",
      "/ Predicting on latest datapredicting tournament era:0809\n",
      "\\ Predicting on latest datapredicting tournament era:0810\n",
      "/ Predicting on latest datapredicting tournament era:0811\n",
      "\\ Predicting on latest datapredicting tournament era:0812\n",
      "/ Predicting on latest datapredicting tournament era:0813\n",
      "\\ Predicting on latest datapredicting tournament era:0814\n",
      "/ Predicting on latest datapredicting tournament era:0815\n",
      "\\ Predicting on latest datapredicting tournament era:0816\n",
      "/ Predicting on latest datapredicting tournament era:0817\n",
      "\\ Predicting on latest datapredicting tournament era:0818\n",
      "/ Predicting on latest datapredicting tournament era:0819\n",
      "\\ Predicting on latest datapredicting tournament era:0820\n",
      "/ Predicting on latest datapredicting tournament era:0821\n",
      "\\ Predicting on latest datapredicting tournament era:0822\n",
      "/ Predicting on latest datapredicting tournament era:0823\n",
      "\\ Predicting on latest datapredicting tournament era:0824\n",
      "/ Predicting on latest datapredicting tournament era:0825\n",
      "\\ Predicting on latest datapredicting tournament era:0826\n",
      "/ Predicting on latest datapredicting tournament era:0827\n",
      "\\ Predicting on latest datapredicting tournament era:0828\n",
      "/ Predicting on latest datapredicting tournament era:0829\n",
      "\\ Predicting on latest datapredicting tournament era:0830\n",
      "/ Predicting on latest datapredicting tournament era:0831\n",
      "\\ Predicting on latest datapredicting tournament era:0832\n",
      "/ Predicting on latest datapredicting tournament era:0833\n",
      "\\ Predicting on latest datapredicting tournament era:0834\n",
      "/ Predicting on latest datapredicting tournament era:0835\n",
      "\\ Predicting on latest datapredicting tournament era:0836\n",
      "/ Predicting on latest datapredicting tournament era:0837\n",
      "\\ Predicting on latest datapredicting tournament era:0838\n",
      "/ Predicting on latest datapredicting tournament era:0839\n",
      "\\ Predicting on latest datapredicting tournament era:0840\n",
      "/ Predicting on latest datapredicting tournament era:0841\n",
      "\\ Predicting on latest datapredicting tournament era:0842\n",
      "/ Predicting on latest datapredicting tournament era:0843\n",
      "\\ Predicting on latest datapredicting tournament era:0844\n",
      "/ Predicting on latest datapredicting tournament era:0845\n",
      "\\ Predicting on latest datapredicting tournament era:0846\n",
      "/ Predicting on latest datapredicting tournament era:0847\n",
      "\\ Predicting on latest datapredicting tournament era:0848\n",
      "/ Predicting on latest datapredicting tournament era:0849\n",
      "\\ Predicting on latest datapredicting tournament era:0850\n",
      "/ Predicting on latest datapredicting tournament era:0851\n",
      "\\ Predicting on latest datapredicting tournament era:0852\n",
      "/ Predicting on latest datapredicting tournament era:X\n",
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_BLITTER4_neutral_riskiest_50 | 0.0217391 | 0.864442 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "validation_data.loc[:, f\"preds_{model_name}\"].hist()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "execution_count": 135
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZcklEQVR4nO3df4zU933n8efroDjYqQ3E6RxiUaEXlIp4r6pZAadUp01IYXFywadzIiwUloSEVrZbt9oqxc2diPxDddr6XLtJXNFADTmfsUtbwdX4CMWMcpWKje04xthxWWNSdoVNYgjuxo19m777x3y2maznsz9mvjM7mNdDGu33+/5+Pp/ve74zs+/9/ppVRGBmZlbLv5vqBMzMrH25SJiZWZaLhJmZZblImJlZlouEmZllTZ/qBIp25ZVXxoIFCwof94c//CGXXXZZ4eMWzXkWy3kWy3kWp+gcn3rqqe9HxHvftiAi3lGPJUuWRDMcOnSoKeMWzXkWy3kWy3kWp+gcgSejxu9UH24yM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzs6x33NdymNnbLdj8yJSs9+SdH52S9VpxvCdhZmZZLhJmZpblImFmZlnjnpOQtB34GHAmIq4atawP+CPgvRHxfUkC7gGuAd4ANkTE06ltL/DfU9fbI2JHii8B7gdmAvuAmyMiJM0BHgIWACeBT0bEuYaerdkUauZ5gb7OYTZM0XkHe2ebyJ7E/UDP6KCk+cBK4B+rwquBRemxCbgvtZ0DbAGWAUuBLZJmpz73AZ+r6jeyrs3AwYhYBBxM82Zm1kLjFomI+CZwtsaiu4HPA1EVWwPsTF9PfhiYJWkusAo4EBFn097AAaAnLbs8Ig6n7zPfCVxbNdaONL2jKm5mZi1S1zkJSWuAwYj49qhF84BTVfMDKTZWfKBGHKAUEafT9CtAqZ5czcysfpO+T0LSpcDvUTnU1BLpHEXklkvaROXwFqVSiXK5XHgOQ0NDTRm3aM6zWEXm2dc5XMg4tZRmNnf8eo3edhfj694srcqxnpvp/gOwEPh25Tw1HcDTkpYCg8D8qrYdKTYIdI+Kl1O8o0Z7gFclzY2I0+mw1JlcQhGxFdgK0NXVFd3d3bmmdSuXyzRj3KI5z2IVmWczTyz3dQ5z19H2uzf25Lrun5q/GF/3ZmlVjpM+3BQRRyPi5yJiQUQsoHKI6OqIeAXYC6xXxXLgfDpktB9YKWl2OmG9Etiflr0uaXm6Mmo9sCetai/Qm6Z7q+JmZtYi4xYJSQ8Cfw+8X9KApI1jNN8HnAD6gT8DbgCIiLPAbcCR9Lg1xUhtvpb6vAQ8muJ3Ar8q6TjwkTRvZmYtNO7+aURcP87yBVXTAdyYabcd2F4j/iRwVY34a8CK8fIzM7Pm8R3XZmaW5SJhZmZZLhJmZpblImFmZlkuEmZmluUiYWZmWS4SZmaW5SJhZmZZLhJmZpblImFmZlkuEmZmluUiYWZmWS4SZmaW5SJhZmZZLhJmZpblImFmZlkuEmZmluUiYWZmWS4SZmaW5SJhZmZZ4xYJSdslnZH0XFXsDyV9R9Kzkv5a0qyqZbdI6pf0oqRVVfGeFOuXtLkqvlDS4yn+kKQZKX5Jmu9PyxcU9aTNzGxiJrIncT/QMyp2ALgqIv4j8A/ALQCSFgNrgQ+kPl+VNE3SNOArwGpgMXB9agvwJeDuiHgfcA7YmOIbgXMpfndqZ2ZmLTRukYiIbwJnR8W+ERHDafYw0JGm1wC7IuLNiHgZ6AeWpkd/RJyIiLeAXcAaSQI+DOxO/XcA11aNtSNN7wZWpPZmZtYi0wsY4zPAQ2l6HpWiMWIgxQBOjYovA94D/KCq4FS3nzfSJyKGJZ1P7b8/OgFJm4BNAKVSiXK53NgzqmFoaKgp4xbNeRaryDz7OofHb1Sn0szmjl+v0dvuYnzdm6VVOTZUJCR9ARgGHigmnfpExFZgK0BXV1d0d3cXvo5yuUwzxi2a8yxWkXlu2PxIIePU0tc5zF1Hi/ibr1gn13X/1PzF+Lo3S6tyrPtdJWkD8DFgRURECg8C86uadaQYmfhrwCxJ09PeRHX7kbEGJE0HrkjtzcysReq6BFZSD/B54OMR8UbVor3A2nRl0kJgEfAEcARYlK5kmkHl5PbeVFwOAdel/r3AnqqxetP0dcBjVcXIzMxaYNw9CUkPAt3AlZIGgC1Urma6BDiQziUfjohfj4hjkh4GnqdyGOrGiPhxGucmYD8wDdgeEcfSKn4X2CXpduBbwLYU3wZ8XVI/lRPnawt4vmZmNgnjFomIuL5GeFuN2Ej7O4A7asT3AftqxE9QufppdPxHwCfGy8/MzJrHd1ybmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWVb7/VNcsyZbMIn/Nd3XOdzU/01t1u68J2FmZlkuEmZmluUiYWZmWeMWCUnbJZ2R9FxVbI6kA5KOp5+zU1yS7pXUL+lZSVdX9elN7Y9L6q2KL5F0NPW5V5LGWoeZmbXORPYk7gd6RsU2AwcjYhFwMM0DrAYWpccm4D6o/MIHtgDLgKXAlqpf+vcBn6vq1zPOOszMrEXGLRIR8U3g7KjwGmBHmt4BXFsV3xkVh4FZkuYCq4ADEXE2Is4BB4CetOzyiDgcEQHsHDVWrXWYmVmL1HsJbCkiTqfpV4BSmp4HnKpqN5BiY8UHasTHWsfbSNpEZc+FUqlEuVye5NMZ39DQUFPGLZrzHF9f5/CE25ZmTq79VGnXPEe/xn5/FqdVOTZ8n0REhKQoIpl61xERW4GtAF1dXdHd3V14DuVymWaMWzTnOb7J3PfQ1znMXUfb/3aids3z5Lrun5r3+7M4rcqx3qubXk2Hikg/z6T4IDC/ql1Hio0V76gRH2sdZmbWIvUWib3AyBVKvcCeqvj6dJXTcuB8OmS0H1gpaXY6Yb0S2J+WvS5pebqqaf2osWqtw8zMWmTc/VNJDwLdwJWSBqhcpXQn8LCkjcB3gU+m5vuAa4B+4A3g0wARcVbSbcCR1O7WiBg5GX4DlSuoZgKPpgdjrMPMzFpk3CIREddnFq2o0TaAGzPjbAe214g/CVxVI/5arXWYmVnr+I5rMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7OshoqEpN+WdEzSc5IelPQuSQslPS6pX9JDkmaktpek+f60fEHVOLek+IuSVlXFe1KsX9LmRnI1M7PJq7tISJoH/CbQFRFXAdOAtcCXgLsj4n3AOWBj6rIROJfid6d2SFqc+n0A6AG+KmmapGnAV4DVwGLg+tTWzMxapNHDTdOBmZKmA5cCp4EPA7vT8h3AtWl6TZonLV8hSSm+KyLejIiXgX5gaXr0R8SJiHgL2JXamplZiygi6u8s3QzcAfwz8A3gZuBw2ltA0nzg0Yi4StJzQE9EDKRlLwHLgC+mPv8rxbcBj6ZV9ETEZ1P8U8CyiLipRh6bgE0ApVJpya5du+p+TjlDQ0O8+93vLnzcojnP8R0dPD/htqWZ8Oo/NzGZgjjPt+ucd0XdfS+Ez1HROX7oQx96KiK6Rsen1zugpNlU/rJfCPwA+Asqh4taLiK2AlsBurq6oru7u/B1lMtlmjFu0Zzn+DZsfmTCbfs6h7nraN0fk5Zxnm93cl133X0vhM9Rq3Js5HDTR4CXI+J7EfH/gb8CPgjMSoefADqAwTQ9CMwHSMuvAF6rjo/qk4ubmVmLNFIk/hFYLunSdG5hBfA8cAi4LrXpBfak6b1pnrT8sagc69oLrE1XPy0EFgFPAEeARelqqRlUTm7vbSBfMzObpLr3+yLicUm7gaeBYeBbVA75PALsknR7im1LXbYBX5fUD5yl8kufiDgm6WEqBWYYuDEifgwg6SZgP5Urp7ZHxLF68zUzs8lr6OBgRGwBtowKn6ByZdLotj8CPpEZ5w4qJ8BHx/cB+xrJ0czM6uc7rs3MLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJcJMzMLMtFwszMslwkzMwsy0XCzMyyXCTMzCzLRcLMzLJcJMzMLKuhIiFplqTdkr4j6QVJ/0nSHEkHJB1PP2entpJ0r6R+Sc9KurpqnN7U/rik3qr4EklHU597JamRfM3MbHIa3ZO4B/i/EfGLwC8BLwCbgYMRsQg4mOYBVgOL0mMTcB+ApDnAFmAZsBTYMlJYUpvPVfXraTBfMzObhLqLhKQrgP8MbAOIiLci4gfAGmBHarYDuDZNrwF2RsVhYJakucAq4EBEnI2Ic8ABoCctuzwiDkdEADurxjIzsxaY3kDfhcD3gD+X9EvAU8DNQCkiTqc2rwClND0POFXVfyDFxooP1Ii/jaRNVPZOKJVKlMvlup9UztDQUFPGLZrzHF9f5/CE25ZmTq79VHGeb9fI++tC+By1KsdGisR04GrgNyLicUn38JNDSwBEREiKRhKciIjYCmwF6Orqiu7u7sLXUS6Xaca4RXOe49uw+ZEJt+3rHOauo418TFrDeb7dyXXddfe9ED5HrcqxkXMSA8BARDye5ndTKRqvpkNFpJ9n0vJBYH5V/44UGyveUSNuZmYtUneRiIhXgFOS3p9CK4Dngb3AyBVKvcCeNL0XWJ+ucloOnE+HpfYDKyXNTiesVwL707LXJS1PVzWtrxrLzMxaoNH9vt8AHpA0AzgBfJpK4XlY0kbgu8AnU9t9wDVAP/BGaktEnJV0G3Aktbs1Is6m6RuA+4GZwKPpYWZmLdJQkYiIZ4CuGotW1GgbwI2ZcbYD22vEnwSuaiRHMzOrn++4NjOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzrIb+x7VZvY4OnmfD5kemOg0zG0fDexKSpkn6lqS/SfMLJT0uqV/SQ5JmpPglab4/LV9QNcYtKf6ipFVV8Z4U65e0udFczcxscoo43HQz8ELV/JeAuyPifcA5YGOKbwTOpfjdqR2SFgNrgQ8APcBXU+GZBnwFWA0sBq5Pbc3MrEUaKhKSOoCPAl9L8wI+DOxOTXYA16bpNWmetHxFar8G2BURb0bEy0A/sDQ9+iPiRES8BexKbc3MrEUaPSfxx8DngZ9N8+8BfhARw2l+AJiXpucBpwAiYljS+dR+HnC4aszqPqdGxZfVSkLSJmATQKlUolwu1/2EcoaGhpoybtEulDxLM6Gvc3j8hlPMeRarlXk28jm4ED5Hrcqx7iIh6WPAmYh4SlJ3YRnVISK2AlsBurq6oru7+HTK5TLNGLdoF0qef/LAHu462v7XTfR1DjvPArUyz5PruuvueyF8jlqVYyOv1geBj0u6BngXcDlwDzBL0vS0N9EBDKb2g8B8YEDSdOAK4LWq+IjqPrm4mZm1QN3nJCLilojoiIgFVE48PxYR64BDwHWpWS+wJ03vTfOk5Y9FRKT42nT100JgEfAEcARYlK6WmpHWsbfefM3MbPKasd/3u8AuSbcD3wK2pfg24OuS+oGzVH7pExHHJD0MPA8MAzdGxI8BJN0E7AemAdsj4lgT8jUzs4xCikRElIFymj5B5cqk0W1+BHwi0/8O4I4a8X3AviJyNDOzyfPXcpiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWZaLhJmZZblImJlZlouEmZlluUiYmVmWi4SZmWW5SJiZWVbdRULSfEmHJD0v6Zikm1N8jqQDko6nn7NTXJLuldQv6VlJV1eN1ZvaH5fUWxVfIulo6nOvJDXyZM3MbHIa2ZMYBvoiYjGwHLhR0mJgM3AwIhYBB9M8wGpgUXpsAu6DSlEBtgDLgKXAlpHCktp8rqpfTwP5mpnZJNVdJCLidEQ8nab/CXgBmAesAXakZjuAa9P0GmBnVBwGZkmaC6wCDkTE2Yg4BxwAetKyyyPicEQEsLNqLDMza4HpRQwiaQHwy8DjQCkiTqdFrwClND0POFXVbSDFxooP1IjXWv8mKnsnlEolyuVy/U8mY2hoqCnjFu1CybM0E/o6h6c6jXE5z2K1Ms9GPgcXwueoVTk2XCQkvRv4S+C3IuL16tMGERGSotF1jCcitgJbAbq6uqK7u7vwdZTLZZoxbtEulDz/5IE93HW0kL9Rmqqvc9h5FqiVeZ5c11133wvhc9SqHBu6uknSz1ApEA9ExF+l8KvpUBHp55kUHwTmV3XvSLGx4h014mZm1iKNXN0kYBvwQkT8z6pFe4GRK5R6gT1V8fXpKqflwPl0WGo/sFLS7HTCeiWwPy17XdLytK71VWOZmVkLNLLf90HgU8BRSc+k2O8BdwIPS9oIfBf4ZFq2D7gG6AfeAD4NEBFnJd0GHEntbo2Is2n6BuB+YCbwaHqYmVmL1F0kIuLvgNx9CytqtA/gxsxY24HtNeJPAlfVm6OZmTXGd1ybmVmWi4SZmWW1/zVzZmZ1WLD5kbr79nUOs6HO/ifv/Gjd621H3pMwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPLcpEwM7MsFwkzM8tykTAzsyzfcX2Ra+Su1Eb0dU7Jas1skrwnYWZmWS4SZmaW5SJhZmZZLhJmZpblImFmZlkuEmZmluUiYWZmWW1fJCT1SHpRUr+kzVOdj5nZxaStb6aTNA34CvCrwABwRNLeiHh+ajMzM6utVTeo1voXq83416ltXSSApUB/RJwAkLQLWAO844pEUW+sRv43r5nZaIqIqc4hS9J1QE9EfDbNfwpYFhE3jWq3CdiUZt8PvNiEdK4Evt+EcYvmPIvlPIvlPItTdI4/HxHvHR1s9z2JCYmIrcDWZq5D0pMR0dXMdRTBeRbLeRbLeRanVTm2+4nrQWB+1XxHipmZWQu0e5E4AiyStFDSDGAtsHeKczIzu2i09eGmiBiWdBOwH5gGbI+IY1OUTlMPZxXIeRbLeRbLeRanJTm29YlrMzObWu1+uMnMzKaQi4SZmWVd9EVC0hxJByQdTz9nZ9r1pjbHJfWm2KWSHpH0HUnHJN1Z1f4SSQ+lrxN5XNKCqcozxe+QdErS0Kj2GyR9T9Iz6fHZNs2z3bbnEklHUz73SlKKf1HSYNX2vKaO3Mb8KpqxtoWkW1L8RUmrJjpmPZqU58m0XZ+R9ORU5inpPZIOSRqS9OVRfWq+/m2YZzmNOfJ+/LlJJxYRF/UD+ANgc5reDHypRps5wIn0c3aang1cCnwotZkB/D9gdZq/AfjTNL0WeGiq8kzLlgNzgaFRfTYAX26H7TlOnu22PZ9IuQp4tOp1/yLwOw3kNQ14CfiF9J76NrB4ItsCWJzaXwIsTONMm8iY7ZBnWnYSuLLA92MjeV4G/Arw66M/I7nXvw3zLANdjeR20e9JUPmajx1pegdwbY02q4ADEXE2Is4BB6jcCf5GRBwCiIi3gKep3MsxetzdwIoG/9qoO8+U3+GION3A+qc6z7bZnpLmApenXAPYmelfj3/7Kpr0nhr5Kppc7tXbYg2wKyLejIiXgf403kTGbIc8m6HuPCPihxHxd8CPqhs36fUvPM+iuEhAqeqX0itAqUabecCpqvmBFPs3kmYB/wU4OLpPRAwD54H3THWeGf9N0rOSdkuaP37zKcmznbbnvDQ9Oj7iprQ9t+cOY41hItsmty3Gyree90Wr8wQI4BuSnlLl63Ya1UieY4051uvfLnmO+PN0qOl/1POHVVvfJ1EUSX8L/Psai75QPRMRIWnS1wRLmg48CNwb6csI69HsPDP+D/BgRLwp6deo/KXy4TbMc9KmKM/7gNuo/LK7DbgL+ExBY18MfiUiBtOx8wOSvhMR35zqpC5g69L2/FngL4FPUdnzmbCLokhExEdyyyS9KmluRJxOu5FnajQbBLqr5juoHOsbsRU4HhF/PKrPfGAgFZErgNemOM9a66zO6WtUjtWPaSrypL225yA/Oaw4Eh9M63y1ah1/BvzNWDlm1jneV9HktsVYfYv+epum5BkRIz/PSPprKodhGikSjeQ51pg1X/82y7N6e/6TpP9NZXtOqkj4cFPlaz5GrlrpBfbUaLMfWClpdjp8sDLFkHQ7lRfrt8YY9zrgsXT8ckryzEm/IEd8HHihgRyblidttD3TYarXJS1Pu+/rR/qP2p7/FXhuknlN5KtocttiL7A2XQWzEFhE5QRrM77epvA8JV2W/uJF0mVUtvdkt1+RedY01uvfTnlKmi7pyjT9M8DHqGd7NnLW+53woHJM7yBwHPhbYE6KdwFfq2r3GSon2PqBT6dYB5XDCi8Az6THZ9OydwF/kdo/AfzCVOWZ4n9A5Tjnv6SfX0zx3weOUbma4hDwi22aZ7ttzy4qH7iXgC/zk28v+DpwFHiWyod6bh25XQP8Qxr7Cyl2K/Dx8bYFlUNpL1H5uvzVY41ZwGen0DypXNnz7fQ41iZ5ngTOAkPp/bh4rNe/nfKkctXTU+m9eAy4h3QV2WQe/loOMzPL8uEmMzPLcpEwM7MsFwkzM8tykTAzsywXCTMzy3KRMDOzLBcJMzPL+lfYNsXsIx5lPQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model_name = f\"BLITTER4\"\r\n",
    "model = create_xgb_ranker(model_name)\r\n",
    "save_prediction(model, model_name, is_xgbRanker= False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No nans in the features this week!\n",
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_BLITTER4_neutral_riskiest_50 | 0.0217391 | 0.864442 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "model_name = f\"blitter5\"\r\n",
    "model = create_catboost_regressor(model_name)\r\n",
    "save_prediction(model, model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No nans in the features this week!\n",
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_blitter5_neutral_riskiest_50 | 0.0233502 |  0.97543 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model_name = f\"blitter6\"\r\n",
    "loss_function = \"QueryRMSE\"\r\n",
    "\r\n",
    "model = create_catboost_ranker(model_name, loss_function)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicting blitter6\n",
      "No nans in the features this week!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "save_prediction(model, model_name, use_pool =True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_blitter6_neutral_riskiest_50 | 0.0173495 | 0.855438 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model_name = f\"blitter7\"\r\n",
    "loss_function = \"PairLogit:max_pairs=100000\"\r\n",
    "\r\n",
    "model = create_catboost_ranker(model_name, loss_function)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicting blitter7\n",
      "model not found, training new one\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be2d018a739543b4bf5922ea2584d8c4"
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "saving new model: blitter7\n",
      "No nans in the features this week!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "save_prediction(model, model_name, use_pool =True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_blitter7_neutral_riskiest_50 | 0.0147554 | 0.726439 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO GroupKfold for non overlapping eras\r\n",
    "# optuna optimize\r\n",
    "\r\n",
    "# stacking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "LTR over slightly neutralized target"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a85be61ea43e8b6e79189e6bca7a99912206a1ace4d1f752775c7cc0873391fd"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('numerai': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}