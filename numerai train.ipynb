{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-31 06:36:36,411 INFO numerapi.utils: target file already exists\n",
      "2021-10-31 06:36:36,413 INFO numerapi.utils: download complete\n",
      "2021-10-31 06:36:37,353 INFO numerapi.utils: starting download\n",
      "numerai_tournament_data_288_int8.parquet:  99%|█████████▊| 574M/582M [00:12<00:00, 50.8MB/s]2021-10-31 06:36:51,235 INFO numerapi.utils: target file already exists\n",
      "2021-10-31 06:36:51,237 INFO numerapi.utils: download complete\n",
      "2021-10-31 06:36:52,147 INFO numerapi.utils: starting download\n",
      "\n",
      "example_predictions_288.parquet:   0%|          | 0.00/33.5M [00:00<?, ?B/s]\n",
      "example_predictions_288.parquet:   0%|          | 2.05k/33.5M [00:00<39:18, 14.2kB/s]\n",
      "example_predictions_288.parquet:   0%|          | 44.0k/33.5M [00:00<03:11, 174kB/s] \n",
      "example_predictions_288.parquet:   0%|          | 95.2k/33.5M [00:00<02:12, 252kB/s]\n",
      "example_predictions_288.parquet:   1%|          | 200k/33.5M [00:00<01:07, 495kB/s] \n",
      "example_predictions_288.parquet:   1%|▏         | 425k/33.5M [00:00<00:32, 1.00MB/s]\n",
      "example_predictions_288.parquet:   2%|▏         | 670k/33.5M [00:00<00:24, 1.31MB/s]\n",
      "example_predictions_288.parquet:   2%|▏         | 807k/33.5M [00:00<00:25, 1.30MB/s]\n",
      "example_predictions_288.parquet:   4%|▍         | 1.42M/33.5M [00:01<00:13, 2.35MB/s]\n",
      "example_predictions_288.parquet:   8%|▊         | 2.69M/33.5M [00:01<00:06, 5.08MB/s]\n",
      "example_predictions_288.parquet:  16%|█▌        | 5.28M/33.5M [00:01<00:02, 10.5MB/s]\n",
      "example_predictions_288.parquet:  19%|█▉        | 6.52M/33.5M [00:01<00:02, 11.0MB/s]\n",
      "example_predictions_288.parquet:  33%|███▎      | 11.1M/33.5M [00:01<00:01, 19.3MB/s]\n",
      "example_predictions_288.parquet:  56%|█████▌    | 18.7M/33.5M [00:01<00:00, 34.7MB/s]\n",
      "example_predictions_288.parquet:  67%|██████▋   | 22.3M/33.5M [00:01<00:00, 34.6MB/s]\n",
      "example_predictions_288.parquet:  81%|████████▏ | 27.3M/33.5M [00:01<00:00, 36.4MB/s]2021-10-31 06:36:54,959 INFO numerapi.utils: target file already exists\n",
      "2021-10-31 06:36:54,961 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\ Reading parquet data"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "example_predictions_288.parquet: 33.5MB [00:02, 11.7MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ Reading parquet data"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "numerai_tournament_data_288_int8.parquet: 582MB [00:22, 25.9MB/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v Reading parquet data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "import gc\n",
    "from numerapi import NumerAPI\n",
    "from halo import Halo\n",
    "from utils import save_model, load_model, neutralize, get_biggest_change_features, validation_metrics, download_data\n",
    "\n",
    "\n",
    "\n",
    "napi = NumerAPI()\n",
    "spinner = Halo(text='', spinner='dots')\n",
    "\n",
    "current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament\n",
    "print(current_round)\n",
    "\n",
    "# read in all of the new datas\n",
    "# tournament data and example predictions change every week so we specify the round in their names\n",
    "# training and validation data only change periodically, so no need to download them over again every single week\n",
    "napi.download_dataset(\"numerai_training_data_int8.parquet\", \"numerai_training_data_int8.parquet\")\n",
    "napi.download_dataset(\"numerai_tournament_data_int8.parquet\", f\"numerai_tournament_data_{current_round}_int8.parquet\")\n",
    "napi.download_dataset(\"numerai_validation_data_int8.parquet\", f\"numerai_validation_data_int8.parquet\")\n",
    "napi.download_dataset(\"example_predictions.parquet\", f\"example_predictions_{current_round}.parquet\")\n",
    "napi.download_dataset(\"example_validation_predictions.parquet\", \"example_validation_predictions.parquet\")\n",
    "\n",
    "spinner.start('Reading parquet data')\n",
    "training_data = pd.read_parquet('numerai_training_data_int8.parquet')\n",
    "tournament_data = pd.read_parquet(f'numerai_tournament_data_{current_round}_int8.parquet')\n",
    "validation_data = pd.read_parquet('numerai_validation_data_int8.parquet')\n",
    "example_preds = pd.read_parquet(f'example_predictions_{current_round}.parquet')\n",
    "validation_preds = pd.read_parquet('example_validation_predictions.parquet')\n",
    "spinner.succeed()\n",
    "\n",
    "EXAMPLE_PREDS_COL = \"example_preds\"\n",
    "validation_data[EXAMPLE_PREDS_COL] = validation_preds[\"prediction\"]\n",
    "\n",
    "TARGET_COL = \"target\"\n",
    "ERA_COL = \"era\"\n",
    "\n",
    "# all feature columns start with the prefix \"feature_\"\n",
    "#feature_cols = [c for c in training_data if c.startswith(\"feature_\")]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "f=['feature_haziest_lifelike_horseback', 'feature_glare_factional_assessment', 'feature_exorbitant_myeloid_crinkle', 'feature_travelled_semipermeable_perruquier', 'feature_branched_dilatory_sunbelt', 'feature_moralistic_heartier_typhoid', 'feature_introvert_symphysial_assegai', 'feature_gullable_sanguine_incongruity', 'feature_agile_unrespited_gaucho', 'feature_canalicular_peeling_lilienthal', 'feature_unvaried_social_bangkok', 'feature_lofty_acceptable_challenge', 'feature_grandmotherly_circumnavigable_homonymity', 'feature_undivorced_unsatisfying_praetorium', 'feature_unaired_operose_lactoprotein']\n",
    "f+=['feature_travelled_semipermeable_perruquier', 'feature_planned_superimposed_bend', 'feature_moralistic_heartier_typhoid', 'feature_crowning_frustrate_kampala', 'feature_unaired_operose_lactoprotein', 'feature_flintier_enslaved_borsch', 'feature_cambial_bigoted_bacterioid', 'feature_jerkwater_eustatic_electrocardiograph', 'feature_unvaried_social_bangkok', 'feature_communicatory_unrecommended_velure', 'feature_lofty_acceptable_challenge', 'feature_grandmotherly_circumnavigable_homonymity', 'feature_antichristian_slangiest_idyllist', 'feature_assenting_darn_arthropod', 'feature_haziest_lifelike_horseback', 'feature_exorbitant_myeloid_crinkle', 'feature_beery_somatologic_elimination', 'feature_silver_handworked_scauper', 'feature_canalicular_peeling_lilienthal', 'feature_undivorced_unsatisfying_praetorium']\n",
    "f+=['feature_glare_factional_assessment', 'feature_travelled_semipermeable_perruquier', 'feature_moralistic_heartier_typhoid', 'feature_stylistic_honduran_comprador', 'feature_crowning_frustrate_kampala', 'feature_unaired_operose_lactoprotein', 'feature_flintier_enslaved_borsch', 'feature_unvaried_social_bangkok', 'feature_apomictical_motorized_vaporisation', 'feature_lofty_acceptable_challenge', 'feature_antichristian_slangiest_idyllist', 'feature_store_apteral_isocheim', 'feature_unforbidden_highbrow_kafir', 'feature_buxom_curtained_sienna', 'feature_haziest_lifelike_horseback', 'feature_exorbitant_myeloid_crinkle', 'feature_silver_handworked_scauper', 'feature_canalicular_peeling_lilienthal', 'feature_introvert_symphysial_assegai', 'feature_univalve_abdicant_distrail', 'feature_undivorced_unsatisfying_praetorium']\n",
    "f+=['feature_glare_factional_assessment', 'feature_unsealed_suffixal_babar', 'feature_travelled_semipermeable_perruquier', 'feature_moralistic_heartier_typhoid', 'feature_twisty_adequate_minutia', 'feature_flintier_enslaved_borsch', 'feature_slack_calefacient_tableau', 'feature_bhutan_imagism_dolerite', 'feature_unvaried_social_bangkok', 'feature_communicatory_unrecommended_velure', 'feature_lofty_acceptable_challenge', 'feature_grandmotherly_circumnavigable_homonymity', 'feature_chuffier_analectic_conchiolin', 'feature_antichristian_slangiest_idyllist', 'feature_unwonted_trusted_fixative', 'feature_haziest_lifelike_horseback', 'feature_exorbitant_myeloid_crinkle', 'feature_beery_somatologic_elimination', 'feature_winsome_irreproachable_milkfish', 'feature_gullable_sanguine_incongruity', 'feature_silver_handworked_scauper', 'feature_canalicular_peeling_lilienthal', 'feature_introvert_symphysial_assegai', 'feature_undivorced_unsatisfying_praetorium']\n",
    "\n",
    "feature_cols = list(set(f))\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def create_lgbm_regressor(model_name:str):\n",
    "    model = load_model(model_name)\n",
    "    if not model:\n",
    "        print(f\"model not found, training new one\")\n",
    "        params = {\"n_estimators\": 2000,\n",
    "                \"learning_rate\": 0.01,\n",
    "                \"max_depth\": 5,\n",
    "                \"num_leaves\": 2 ** 5,\n",
    "                \"colsample_bytree\": 0.1}\n",
    "\n",
    "        model = LGBMRegressor(**params)\n",
    "\n",
    "        # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\n",
    "        spinner.start('Training model')\n",
    "        model.fit(training_data.loc[:, feature_cols], training_data[TARGET_COL])\n",
    "        print(f\"saving new model: {model_name}\")\n",
    "        save_model(model, model_name)\n",
    "        spinner.succeed()\n",
    "\n",
    "    # check for nans and fill nans\n",
    "    if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\n",
    "        cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\n",
    "        total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\n",
    "        print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\n",
    "        print(f\"out of {total_rows} total rows\")\n",
    "        print(f\"filling nans with 0.5\")\n",
    "        tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\n",
    "    else:\n",
    "        print(\"No nans in the features this week!\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def save_prediction(model, model_name, neutralize_proportion = 0.8, is_xgbRanker = False, use_pool = False):\n",
    "    spinner.start('Predicting on latest data')\n",
    "    # double check the feature that the model expects vs what is available\n",
    "    # this prevents our pipeline from failing if Numerai adds more data and we don't have time to retrain!\n",
    "\n",
    "\n",
    "    try:\n",
    "        model_expected_features = model.booster_.feature_name()\n",
    "    except:\n",
    "        try:\n",
    "            model_expected_features = model.get_booster().feature_names\n",
    "        except:\n",
    "            model_expected_features = model.feature_names_\n",
    "        \n",
    "    if set(model_expected_features) != set(feature_cols):\n",
    "        print(f\"New features are available! Might want to retrain model {model_name}.\")\n",
    "\n",
    "    if not use_pool:\n",
    "        # if is_xgbRanker:\n",
    "            \n",
    "        #     for era in validation_data.era.unique():\n",
    "        #         print(f\"predicting valid era:{era}\")\n",
    "        #         validation_data.loc[validation_data.era == era, f\"preds_{model_name}\"] = \\\n",
    "        #             model.predict(validation_data.loc[validation_data.era == era, model_expected_features])\n",
    "\n",
    "        #     for era in tournament_data.era.unique():\n",
    "        #         print(f\"predicting tournament era:{era}\")\n",
    "        #         tournament_data.loc[tournament_data.era == era, f\"preds_{model_name}\"] = \\\n",
    "        #             model.predict(tournament_data.loc[tournament_data.era == era, model_expected_features])\n",
    "        # else:\n",
    "        validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(validation_data.loc[:, model_expected_features])\n",
    "        tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(tournament_data.loc[:, model_expected_features])\n",
    "    else:\n",
    "        #catboost ranker is not compayible with sklearn\n",
    "\n",
    "        test_pool = Pool(\n",
    "            data=validation_data.loc[:, feature_cols],\n",
    "            label=validation_data[TARGET_COL],\n",
    "            group_id=validation_data.era\n",
    "        )\n",
    "        validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(test_pool)\n",
    "\n",
    "        tournament_pool = Pool (\n",
    "            data = tournament_data.loc[:, model_expected_features],\n",
    "            group_id = tournament_data.era\n",
    "        )\n",
    "\n",
    "        tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(tournament_pool)\n",
    "\n",
    "    spinner.succeed()\n",
    "\n",
    "    spinner.start('Neutralizing to risky features')\n",
    "    # getting the per era correlation of each feature vs the target\n",
    "    all_feature_corrs = training_data.groupby(ERA_COL).apply(lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\n",
    "\n",
    "    # find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\n",
    "    riskiest_features = get_biggest_change_features(all_feature_corrs, 50)\n",
    "\n",
    "    # neutralize our predictions to the riskiest features\n",
    "    validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=validation_data,\n",
    "                                                                            columns=[f\"preds_{model_name}\"],\n",
    "                                                                            neutralizers=riskiest_features,\n",
    "                                                                            proportion=neutralize_proportion,\n",
    "                                                                            normalize=True,\n",
    "                                                                            era_col=ERA_COL)\n",
    "\n",
    "    tournament_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=tournament_data,\n",
    "                                                                            columns=[f\"preds_{model_name}\"],\n",
    "                                                                            neutralizers=riskiest_features,\n",
    "                                                                            proportion=neutralize_proportion,\n",
    "                                                                            normalize=True,\n",
    "                                                                            era_col=ERA_COL)\n",
    "    spinner.succeed()\n",
    "\n",
    "    model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\n",
    "\n",
    "    # rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\n",
    "    validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n",
    "    tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\n",
    "    validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}_{model_to_submit}.csv\")\n",
    "    tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}_{model_to_submit}.csv\")\n",
    "\n",
    "    # get some stats about each of our models to compare...\n",
    "    # fast_mode=True so that we skip some of the stats that are slower to calculate\n",
    "    validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\n",
    "    print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Blitter4 XGBRank\n",
    "from xgboost import XGBRanker\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def create_xgb_ranker(model_name:str):\n",
    "    model = load_model(model_name)\n",
    "    if not model:\n",
    "        print(f\"model not found, training new one\")\n",
    "\n",
    "                        \n",
    "        model = XGBRanker(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\n",
    "        # cdf = training_data.groupby('era').agg(['count'])\n",
    "        # group = cdf[cdf.columns[0]].values\n",
    "        # del cdf\n",
    "        group = Counter(training_data.era).values()\n",
    "\n",
    "        # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\n",
    "        spinner.start('Training model')\n",
    "        model.fit(training_data.loc[:, feature_cols], training_data[TARGET_COL], group=group)\n",
    "        print(f\"saving new model: {model_name}\")\n",
    "        save_model(model, model_name)\n",
    "        spinner.succeed()\n",
    "\n",
    "    # check for nans and fill nans\n",
    "    if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\n",
    "        cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\n",
    "        total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\n",
    "        print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\n",
    "        print(f\"out of {total_rows} total rows\")\n",
    "        print(f\"filling nans with 0.5\")\n",
    "        tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\n",
    "    else:\n",
    "        print(\"No nans in the features this week!\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "def create_xgb_regressor(model_name:str):\n",
    "    model = load_model(model_name)\n",
    "    if not model:\n",
    "        print(f\"model not found, training new one\")\n",
    "        params = {\"n_estimators\": 2000,\n",
    "                \"learning_rate\": 0.01,\n",
    "                \"max_depth\": 5,\n",
    "                \"num_leaves\": 2 ** 5,\n",
    "                \"colsample_bytree\": 0.1}\n",
    "\n",
    "        model = XGBRegressor(max_depth=5, learning_rate=0.01, \\\n",
    "                        n_estimators=2000, colsample_bytree=0.1) #provar regularitzacions\n",
    "\n",
    "        # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\n",
    "        spinner.start('Training model')\n",
    "        model.fit(training_data.loc[:, feature_cols], training_data[TARGET_COL])\n",
    "        print(f\"saving new model: {model_name}\")\n",
    "        save_model(model, model_name)\n",
    "        spinner.succeed()\n",
    "\n",
    "    # check for nans and fill nans\n",
    "    if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\n",
    "        cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\n",
    "        total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\n",
    "        print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\n",
    "        print(f\"out of {total_rows} total rows\")\n",
    "        print(f\"filling nans with 0.5\")\n",
    "        tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\n",
    "    else:\n",
    "        print(\"No nans in the features this week!\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#BLITTER5 - Catboost regressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def create_catboost_regressor(model_name:str):\n",
    "\n",
    "    model = load_model(model_name)\n",
    "    if not model:\n",
    "        print(f\"model not found, training new one\")\n",
    "\n",
    "        model = CatBoostRegressor(max_depth=5, learning_rate=0.01, \\\n",
    "                        n_estimators=2000, rsm=0.1) #provar regularitzacions\n",
    "\n",
    "        # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\n",
    "        spinner.start('Training model')\n",
    "        model.fit(training_data.loc[:, feature_cols], training_data[TARGET_COL])\n",
    "        print(f\"saving new model: {model_name}\")\n",
    "        save_model(model, model_name)\n",
    "        spinner.succeed()\n",
    "\n",
    "    # check for nans and fill nans\n",
    "    if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\n",
    "        cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\n",
    "        total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\n",
    "        print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\n",
    "        print(f\"out of {total_rows} total rows\")\n",
    "        print(f\"filling nans with 0.5\")\n",
    "        tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\n",
    "    else:\n",
    "        print(\"No nans in the features this week!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "def create_catboost_ranker(model_name : str, loss_function : str, plot = True, verbose = False):\n",
    "    gc.collect()\n",
    "    print(f\"predicting {model_name}\")\n",
    "    model = load_model(model_name)\n",
    "    if not model:\n",
    "        print(f\"model not found, training new one\")\n",
    "        parameters = {\n",
    "            'iterations': 200,\n",
    "            'custom_metric': ['NDCG', 'PFound'],\n",
    "            'verbose': verbose,\n",
    "            'random_seed': 0,\n",
    "            'rsm' : 0.1,\n",
    "            'learning_rate' : 0.01,\n",
    "            'loss_function' : loss_function,\n",
    "            #'train_dir' : loss_function\n",
    "        }\n",
    "\n",
    "        train_pool = Pool(\n",
    "            data=training_data.loc[:, feature_cols],\n",
    "            label=training_data[TARGET_COL],\n",
    "            group_id=training_data.era\n",
    "        )\n",
    "\n",
    "        test_pool = Pool(\n",
    "            data=validation_data.loc[:, feature_cols],\n",
    "            label=validation_data[TARGET_COL],\n",
    "            group_id=validation_data.era\n",
    "        )\n",
    "\n",
    "        model = CatBoostRanker(**parameters)\n",
    "\n",
    "\n",
    "        # train on all of train, predict on val, predict on tournament, save the model so we don't have to train next time\n",
    "        #spinner.start('Training model')\n",
    "        model.fit(train_pool, eval_set=test_pool, early_stopping_rounds=10, plot=plot)\n",
    "\n",
    "        print(f\"saving new model: {model_name}\")\n",
    "        save_model(model, model_name)\n",
    "        #spinner.succeed()\n",
    "\n",
    "    # check for nans and fill nans\n",
    "    if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\n",
    "        cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\n",
    "        total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\n",
    "        print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\n",
    "        print(f\"out of {total_rows} total rows\")\n",
    "        print(f\"filling nans with 0.5\")\n",
    "        tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\n",
    "    else:\n",
    "        print(\"No nans in the features this week!\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nans in the features this week!\n",
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                   |      mean |   sharpe |\n",
      "|:----------------------------------|----------:|---------:|\n",
      "| preds_blitter_neutral_riskiest_50 | 0.0228259 |  1.05479 |\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"blitter\"\n",
    "model = create_lgbm_regressor(model_name)\n",
    "save_prediction(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nans in the features this week!\n",
      "\\ Predicting on latest data"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marc\\.conda\\envs\\numerai\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_blitter1_neutral_riskiest_50 | 0.0241834 |  1.04763 |\n"
     ]
    }
   ],
   "source": [
    "model_name = \"blitter1\" #f\"xgboost_bare\"\n",
    "model = create_xgb_regressor(model_name)\n",
    "save_prediction(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_BLITTER2_neutral_riskiest_50 | 0.0239164 |  1.06938 |\n"
     ]
    }
   ],
   "source": [
    "#Ensemble Lgbm + XGBoost ja neutralitzzades\n",
    "model_name = \"BLITTER2\"\n",
    "\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = validation_data[\"preds_blitter_neutral_riskiest_50\"] * 0.5 + validation_data[\"preds_blitter1_neutral_riskiest_50\"] * 0.5\n",
    "tournament_data.loc[:, f\"preds_{model_name}\"] = tournament_data[\"preds_blitter_neutral_riskiest_50\"] * 0.5 + tournament_data[\"preds_blitter1_neutral_riskiest_50\"] * 0.5\n",
    "\n",
    "model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\n",
    "\n",
    "validation_data[model_to_submit] = validation_data[f\"preds_{model_name}\"]\n",
    "tournament_data[model_to_submit] = tournament_data[f\"preds_{model_name}\"]\n",
    "\n",
    "\n",
    "# rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\n",
    "\n",
    "validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}_{model_to_submit}.csv\")\n",
    "tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}_{model_to_submit}.csv\")\n",
    "\n",
    "validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                |      mean |   sharpe |\n",
      "|:---------------|----------:|---------:|\n",
      "| preds_BLITTER3 | 0.0236422 |  1.06787 |\n"
     ]
    }
   ],
   "source": [
    "#Ensemble Lgbm + XGBoost ensemble after rank\n",
    "model_name = \"BLITTER3\"\n",
    "\n",
    "#ensemble on ranks\n",
    "validation_data.loc[:, f\"preds_{model_name}\"] = validation_data[\"preds_blitter_neutral_riskiest_50\"].rank(pct=True) + validation_data[\"preds_blitter1_neutral_riskiest_50\"].rank(pct=True) / 2\n",
    "tournament_data.loc[:, f\"preds_{model_name}\"] = tournament_data[\"preds_blitter_neutral_riskiest_50\"].rank(pct=True) + tournament_data[\"preds_blitter1_neutral_riskiest_50\"].rank(pct=True) / 2\n",
    "\n",
    "model_to_submit = f\"preds_{model_name}\"\n",
    "\n",
    "#rank again to fix decimals coming from the /2\n",
    "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n",
    "tournament_data[\"prediction\"] = tournament_data[model_to_submit].rank(pct=True)\n",
    "\n",
    "\n",
    "validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}_{model_to_submit}.csv\")\n",
    "tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}_{model_to_submit}.csv\")\n",
    "\n",
    "validation_stats = validation_metrics(validation_data, [model_to_submit], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\n",
    "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nans in the features this week!\n",
      "\\"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marc\\.conda\\envs\\numerai\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_BLITTER4_neutral_riskiest_50 | 0.0217391 | 0.864442 |\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"BLITTER4\"\n",
    "model = create_xgb_ranker(model_name)\n",
    "save_prediction(model, model_name, is_xgbRanker= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "validation_data.loc[:, f\"preds_{model_name}\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nans in the features this week!\n",
      "\\"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marc\\.conda\\envs\\numerai\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_BLITTER4_neutral_riskiest_50 | 0.0217391 | 0.864442 |\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"BLITTER4\"\n",
    "model = create_xgb_ranker(model_name)\n",
    "save_prediction(model, model_name, is_xgbRanker= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nans in the features this week!\n",
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_blitter5_neutral_riskiest_50 | 0.0233502 |  0.97543 |\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"blitter5\"\n",
    "model = create_catboost_regressor(model_name)\n",
    "save_prediction(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting blitter6\n",
      "No nans in the features this week!\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"blitter6\"\n",
    "loss_function = \"QueryRMSE\"\n",
    "\n",
    "model = create_catboost_ranker(model_name, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_blitter6_neutral_riskiest_50 | 0.0173495 | 0.855438 |\n"
     ]
    }
   ],
   "source": [
    "save_prediction(model, model_name, use_pool =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting blitter7\n",
      "No nans in the features this week!\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"blitter7\"\n",
    "loss_function = \"PairLogit:max_pairs=100000\"\n",
    "\n",
    "model = create_catboost_ranker(model_name, loss_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v Predicting on latest data\n",
      "v Neutralizing to risky features\n",
      "|                                    |      mean |   sharpe |\n",
      "|:-----------------------------------|----------:|---------:|\n",
      "| preds_blitter7_neutral_riskiest_50 | 0.0147554 | 0.726439 |\n"
     ]
    }
   ],
   "source": [
    "save_prediction(model, model_name, use_pool =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO GroupKfold for non overlapping eras\n",
    "# optuna optimize\n",
    "\n",
    "# stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LTR over slightly neutralized target"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12eb2deb491f06237ee9901a58432554b7037b679c06f592229f3038c962b482"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('numerai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
